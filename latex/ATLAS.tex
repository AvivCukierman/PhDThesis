\newcommand{\AtlasCoordFootnote}{
ATLAS uses a right-handed coordinate system with its origin at the nominal interaction point (IP)
in the center of the detector and the $z$-axis along the beam pipe.
The $x$-axis points from the IP to the center of the LHC ring,
and the $y$-axis points upwards.
Cylindrical coordinates $(r,\phi)$ are used in the transverse plane, 
$\phi$ being the azimuthal angle around the $z$-axis.
The pseudorapidity is defined in terms of the polar angle $\theta$ as $\eta = -\ln \tan(\theta/2)$.
Angular distance is measured in units of $\Delta R \equiv \sqrt{(\Delta\eta)^{2} + (\Delta\phi)^{2}}$.}

\section{Introduction}
The ATLAS detector~\cite{PERF-2007-01} is a general-purpose particle physics detector 
with nearly $4\pi$ coverage in solid angle around the collision point.\footnote{\AtlasCoordFootnote}
The physics results that ATLAS produces are enabled not only by the hardware that measures the properties of outgoing particles, but also by software which simulates, stores, and processes this enormous amount of data.

The detector itself is designed to have many concentric layers serving different purposes, in particular to detect and identify all kinds of particles that may be encountered from the collisions at the LHC (Section~\ref{sec:ATLAS:ATLAS}).

The physics phenomena that occur in the $pp$ collisions provided by the LHC and their subsequent interactions with the ATLAS detector are simulated using a variety of generators and a detailed detector simulation (Section~\ref{sec:ATLAS:simulation}).

Because of the extremely high rate of events and large amount of data read out per event, there is a trigger system which lowers the rate that is written to disk (Section~\ref{sec:ATLAS:trigger}).

Finally, the readouts of all the various detector subsystems are combined to construct objects intended to match individual Standard Model particles (Section~\ref{sec:ATLAS:objects}).
Tracks and calorimeter clusters are combined to form detector-level photons, electrons, taus, jets (Chapter~\ref{ch:Jets}), and muons.
All of these objects are taken together to calculate the missing energy in the event which could be due to neutrinos or beyond-the-Standard-Model physics.

\section{Hardware}
\label{sec:ATLAS:ATLAS}
As mentioned above, the ATLAS detector (A Toroidal LHC ApparatuS) is a general purpose particle physics detector built around the nominal interaction point for $pp$ collisions provided by the LHC\footnote{This Section is sourced mainly from the description of ATLAS in~\cite{PERF-2007-01}; however, the Author is also grateful for the additional explanations that can be found in previous SLAC ATLAS students' PHD theses, in particular~\cite{Swiatlowski:2040684} and~\cite{Nachman:2016qyc}. The later Sections of this Chapter also benefit from these sources.}.
It is an enormous instrument, roughly cylindrical with a diameter of about 25 m and a length of about 44 m.
A cutout of the ATLAS detector with some parts labeled can be seen in Figure~\ref{fig:ATLAS:ATLAS}.

\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.8\textwidth]{figures/{figures_AtlasDetectorLabelled.png}}}
  \caption{A cutout view of the ATLAS detector with major subsystems labeled. People included for scale. Figure sourced from~\cite{PERF-2007-01}.}
  \label{fig:ATLAS:ATLAS}
\end{figure}

ATLAS consists of an inner tracking detector surrounded by a superconducting solenoid providing a \SI{2}{\tesla} axial magnetic field (Section~\ref{sec:ATLAS:tracker}), a system of calorimeters (Section~\ref{sec:ATLAS:calorimeter}), and a muon spectrometer incorporating three large superconducting toroid magnets (Section~\ref{sec:ATLAS:MS}).
The various layers target different kinds of particles based on their interactions with matter, as can be seen in Figure~\ref{fig:ATLAS:schematic}.

\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{ATLAS_schematic.jpg}}}
  \caption{A schematic of various particles passing through the ATLAS detector. Figure sourced from ~\cite{Pequenao:1096081}.}
  \label{fig:ATLAS:schematic}
\end{figure}

This schematic only shows the ideal or targeted case; in practice the object identification is a non-trivial problem (Section~\ref{sec:ATLAS:objects}).
Charged particles leave tracks in the tracker, which is surrounded by a solenoid magnet to bend their tracks and measure charge and momentum.
Photons and electrons are stopped and their energies measured in the electromagnetic calorimeter, while hadrons interact to a lesser extent and are ultimately stopped and measured in the hadronic calorimeter.
Muons pass through the entire calorimeter system and are measured in the muon system, which is surrounded by superconducting toroids.
Finally, neutrinos interact only very weakly with matter and pass right through the detector; these can only be reconstructed as missing energy and momentum in the event.

\subsection{Tracker}
\label{sec:ATLAS:tracker}
The tracker, or inner detector (as it is the closest part of ATLAS to the beamline), consists of 3 layers with different technologies: the silicon pixel detectors, the semiconductor strip tracker (SCT), and the transition radiation tracker (TRT).
The entire tracking system is immersed in a 2 T magnetic field provided by a surrounding solenoid.
These components, when taken together, provide charged-particle tracking in the range $|\eta| < 2.5$.
A cutout view of the tracking system can be seen in Figure~\ref{fig:ATLAS:tracker}.
\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{ATLAS_tracker}.png}}
  \caption{A cutout view of the tracking system showing the various layers. Figure sourced from~\cite{ATL-PHYS-PUB-2015-018}.}
  \label{fig:ATLAS:tracker}
\end{figure}

The pixel detectors are the innermost part of the ATLAS detector to the beamline.
The original design included 3 layers, although a 4th layer, the insertable $B$-layer (IBL), was installed between Run 1 and Run $2$ in 2014.
The pixel detectors are composed of silicon and operate as ionizing radiation detectors.
As charged particles pass through the material, electrons are knocked loose and these are measured in each individual pixel, without substantially affecting the momentum of the charged particle.
The pixel system has very good spatial resolution, with each pixel having a size of $50\times 400$ $\mu\text{m}^2$ in the outer 3 layers and $50\times 250$ $\mu\text{m}^2$ in the IBL.
There is both a cylindrical set of pixel detectors in the barrel and an endcap set on the ends.
The accuracies of the pixels are $10\times 115$ $\mu\text{m}^2$ in $R-\phi \times z$ in the barrel and also $10\times 115$ $\mu\text{m}^2$ in $R-\phi \times R$ in the endcaps.
There are roughly 80.4 million independent pixel channels.

The next outermost layer is the SCT, which operates under very similar principles to the pixel detectors.
However, insted of small pixels long and thin strips are used, which provide spatial information in only one direction.
Because of this, each of the 4 layers of the SCT are actually composed of 2 layered and slightly offset strips at an angle of 40 mrad to each other, in order to get a (rough) measurement in the second direction as well.
In the barrel region these strips are parallel to the beam direction; in the endcaps they are radial.
The strips have an accuracy of $17\times 580$ $\mu\text{m}$ in $R-\phi \times z$ in the barrel and also $17\times 580$ $\mu\text{m}$ in $R-\phi \times R$ in the endcaps.
There are approximately 6.3 million readout channels in the SCT.

The final layer consists of the TRT, which is composed of $4$ mm (in diameter) drift tubes.
These drift tubes are not made out of silicon, but rather are filled with gas which, as charged particles pass through, gets ionized; this signal is amplified by a large voltage difference (1530 V) between the center and exterior of the tube.
The tubes do not provide any $z$ resolution, but only information in $R-\phi$ with an accuracy of 130 $\mu\text{m}$, both in the barrel and endcap regions.
However, this is mitigated somewhat by every particle passing through about 30 tubes before exiting the tracker.
There are approximately 351000 readout channels in the TRT.

The entire tracking system is surrounded by a superconducting solenoid which generates an axial field of 2 T.
As charged particles pass through this magnetic field, their total momentum is not changed as magnetic fields do no work, but the direction of the momentum curves in the $\phi$ direction with the radius of curvature determined by the charge-to-(transverse) momentum ratio.

The individual hits in the various layers are combined together in software to identify paths of charged particles through the tracker and magnetic field, measuring both the charge and momentum of the charged particle (Section~\ref{sec:ATLAS:tracks}).

\subsection{Calorimeters}
\label{sec:ATLAS:calorimeter}
The ATLAS calorimeter system is designed to stop and measure the energy of all charged and neutral particles that exit the tracker other than muons and neutrinos.
These calorimeters cover the region $|\eta| < 4.9$, in comparison to the tracker which only covers $|\eta|<2.5$. 
The calorimeter system consists of two main subsystems: the inner electromagnetic calorimeter, which is intended to interact electromagnetically and measure particles like photons and electrons, and the outer hadronic calorimeter, which is intended to interact both electromagnetically and via nuclear interactions in order to measure hadronic particles.
A cutout view of the calorimeter system can be seen in Figure~\ref{fig:ATLAS:calorimeters}.
\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.8\textwidth]{figures/{ATLAS_calorimeters}.png}}
  \caption{A cutout view of the calorimeter system showing the various subsystems. Figure sourced from~\cite{PERF-2007-01}.}
  \label{fig:ATLAS:calorimeters}
\end{figure}

Both subsystems are sampling calorimeters which operate under the principle of alternating passive and active layers.
The passive layers are made of some dense material (lead, steel, copper, or tungsten) that have high probability of interacting with the energetic particles passing through them, causing a cascade of lower energy radiation that is easier to measure (\textit{sample}) in the active materials.
The active materials use ionization or scintillation to measure the energies of the particles passing through them.
In ioniziation, the material is ionized by electrons being knocked free; the free electrons are then drifted to the side of the cell and measured.
In scintillation, excited molecules emit photon radiation which can then be read out by photomultiplier tubes.
All systems are non-compensating, meaning they do not account for energy loss in the passive layers; this is one cause for the need to calibrate the energy of physics objects formed in the calorimeter, in particular jets (Section~\ref{sec:ATLAS:jet_calibration}).
However, the innermost layer of electromagnetic calorimeter is a presampler which does compensate for the energy loss in the tracker.

High energy charged electrons and positrons interacting with nuclei in the materials are dominated by bremsstrahlung, which is photon radiation due to deceleration in the material~\cite{Lechner:2674116}.
High energy photons, in turn, primarily convert to electron-positron pairs in the field of nuclei, which further lose energy to bremsstrahlung; this back-and-forth is referred to as an \textit{electromagnetic shower}.
Below a certain energy (depending on the material, but usually some MeV), other processes take over.
Muons can be considered to be heavy electrons; however, because of their higher mass, bremmstrahlung does not dominate until the muon energy is above $\sim 1000$ GeV~\cite{TASI_day3_school}; thus muons tend to pass through the entire calorimeter without losing much energy and can only be measured in the muon spectrometer (Section~\ref{sec:ATLAS:MS}).

The loss of energy in an electromagnetic shower at high energies can be characterized as
\begin{align}
\frac{dE}{dx} = -\frac{E}{X_0},
\label{eqn:ATLAS:radiation}
\end{align}
where $X_0$ is called the \textit{radiation length}, and is characteristic of the interacting material. 
The solution to~\ref{eqn:ATLAS:radiation} implies an exponential loss of energy as a function of distance in the material, meaning the shower length is logarithmic in initial energy and also that a fixed size detector can cover many orders of magnitude of energy.

For nuclear interactions the underlying processes are more complicated, but the \textit{hadronic interaction length} $\lambda$ gives a similar length scale for hadrons passing through a material and forming \textit{hadronic showers}~\cite{TASI_day3_school} of pions, photons, and positrons/electrons.
For dense materials $\lambda > X_0$ by a factor of 5-10~\cite{Lechner:2674116}, implying that hadronic showers are much longer and occur later than electromagnetic showers; this is why the hadronic calorimeter is outside the electromagnetic calorimeter.

The electromagnetic calorimeter is broken down into the barrel, which covers $|\eta|<1.475$, and two end-caps which cover $1.375<|\eta|<3.2$.
Each of these have three layers in addition to the innermost presampler layer.
Both of these use lead as the passive material and liquid argon (LAr) as the active material, which measures the energy of particles via ionization.
The total thickness of the electromagnetic calorimeter is $>22 X_0$ in the barrel and $>24 X_0$ in the endcaps.
The size of the calorimeter cells vary with $\eta$ and depth, but the smallest cells, which occur in the second layer, are $0.025 \times 0.025$ in $\Delta\eta \times \Delta\phi$.

The hadronic calorimeter consists of the barrel, which covers $|\eta|<1.7$, two end-caps which cover $1.5<|\eta|<3.2$, and two forward calorimeters which cover $3.1<|\eta|<4.9$.
These three components consist of 3, 4, and 3 independent layers respectively.
The barrel uses steel as the passive material and scintillating tiles as the active material; however, the end-caps and forward calorimeters use ionizing LAr as the active material, with copper (in the end-caps and the first layer of the forward calorimeter) and tungsten (in the second and third layers of the forward calorimeter) as the passive materials.
The total thickness of the hadronic calorimeter is $\gtrsim 10\lambda$ over the entire detector.
The smallest calorimeter cells in the barrel and end-caps are $0.1 \times 0.1$ in $\Delta\eta \times \Delta\phi$.
In the forward calorimeters, $\eta$ increases rapidly with $\theta$, so the sizes are simply measured on an absolute scale, with the smallest cells $3.0 \times 2.6$ $\text{cm}^2$ in $\Delta x \times \Delta y$.

\subsection{Muon System}
\label{sec:ATLAS:MS}
The outermost radial component of the detector is the muon system, or muon spectrometer (MS).
In principle due to the interactions with the calorimeters the only particles that can make it out so far are muons; the MS provides measurements of muon tracks out to $|\eta|<2.7$, with an additional triggering system that goes out to $|\eta|<2.4$.
However, very energetic hadrons can ``punch through'' to the MS; the energy of jets therefore not measured in the calorimeter can be corrected (Section~\ref{sec:ATLAS:jet_calibration}).
A cutout view of the MS can be seen in Figure~\ref{fig:ATLAS:MS}.
\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.8\textwidth]{figures/{ATLAS_MS}.png}}
  \caption{A cutout view of the muon system showing the various subsystems. Figure sourced from~\cite{PERF-2007-01}.}
  \label{fig:ATLAS:MS}
\end{figure}

The MS is surrounded by a toroid magnet system which bends muons in the $\pm z/\eta$ direction in the barrel.
In the barrel, there are eight toroids arranged symmetrically about the beam axis; there are in addition two end-cap toroids.

The main component of the MS are the monitored drift tubes (MDT), which operate similarly to the TRTs in Section~\ref{sec:ATLAS:tracker}.
These cover the region $|\eta|<2.7$, with three layers out to $|\eta|<2.0$ and two beyond that.
As drift tubes, they provide a resolution of $35$ $\mu\text{m}$ in the $z$ direction and no measurement in the $\phi$ direction; this choice of orientation is intended to measure the trajectory of the muons in the magnetic field and therefore their momentum.
In addition, the drift time in the MDTs is large (approximately $700$ ns) relative to the frequency of bunch crossings ($25$ ns), so that other systems must be used to trigger (Section~\ref{sec:ATLAS:trigger}) on muons.

Cathode-strip chambers (CSC) provide tracking measurements in the end-cap region $2.0<|\eta|<2.7$ with alternating layers of perpendicular strips.
The CSCs are multi-wire proportional chambers which drift electrons from the inside to the outside of the chamber.
Because of the perpendicular strips, the CSCs provide measurements in both directions, with a resolution of $40$ $\mu\text{m}$ $\times$ $5$ $\text{mm}$ in $R \times \phi$.

The muon triggering system is provided by the resistive plate chambers (RPC) in the barrel region $|\eta|<1.05$, and by thin-gap chambers (TGC) in the end-cap region $1.05<|\eta|<2.4$.
The RPCs are parallel plate capacitors filled with gas which are segmented in order to provide measurements in both directions, $10\times 10$ $\text{mm}^2$ in $z \times \phi$.
The drift time in the parallel plates is significantly less than in the MDTs, allowing for use in triggering.
The TGCs are multi-wire proportional chambers similar to the CSCs, and provide resolution of about $5\times 5$ $\text{mm}^2$ in $R \times \phi$.

\section{Simulation}
\label{sec:ATLAS:simulation}
Every analysis in ATLAS relies on simulated events in some way.
Both of the analyses presented in this Thesis (Chapter~\ref{ch:HBSM} and Chapter~\ref{ch:CWoLa}) are searches for new BSM physics; since BSM particles have never been observed, simulations are required to understand the sensitivity of the analysis to these new signals.
Many analyses in ATLAS use simulations to model their background - both of these searches avoid this by estimating the background in a data-driven way, but still require simulations of the background in order to set up and validate the analysis chain.
Furthermore, a major step of object calibrations is understanding the effect of the detector in simulated events, which is the subject of Chapters~\ref{ch:NI} and~\ref{ch:GenNI}.
These are just some of the ways that simulations are used in ATLAS.

Simulations of physics events are \textit{factorized}, often using different software programs entirely for matrix element calculations and parton showering, fragmentation and hadronization, and detector simulation~\cite{powhegintro}.
Fundamentally each step of this process is random due to quantum effects, so events are simulated via Monte Carlo, or \textit{MC}, methods to fully populate the relevant probability distributions.
Since even the most common ``interesting'' physics processes have cross sections on the order of $10^{-5}$ of the total $pp$ cross section, and many other important processes are much rarer still, events are typically simulated not inclusively but rather by first specifying the underlying \textit{hard-scatter} (i.e., the tree-level $pp$ interaction) process and proceeding from there.

The matrix element calculations cover the initial $2\rightarrow N$ interactions of the constituent quarks in the colliding protons at the very high energies of the beams.
The parton showering further simulates the produced partons as they radiate down to the hadronization scale around $\mathcal{O}($GeV$)$.
Because the energies in these steps are above the hadronization scale, they can be calculated perturbatively at fixed order in $\alpha_s$, and the generators are specified as to what order they go to (leading order ``LO'', next-to-leading order ``NLO'', etc.).
The three most common generators in use are \PYTHIA~\cite{Sjostrand:2007gs,Sjostrand:2014zea}, \SHERPA~\cite{Gleisberg:2008ta}, and \HERWIG~\cite{Bahr:2008pv,Bellm:2015jjp}.
Each of these can use \POWHEGBOX~\cite{Frixione:2007vw,Alioli:2010xd} to interface between the matrix element and parton showering.
Often a single generator will be used primarily for the simulated events in an analysis, and one or multiple other generators will be used to estimate the uncertainties related to the theoretical calculations.

The fragmentation and hadronization covers the conversion of partons to hadrons.
This process is not described well by any fundamental theory, and so the generators use phenomenological models that are tuned to empirical data (e.g.,~\cite{ATL-PHYS-PUB-2014-021}); similarly, generators are specified by this hadronization tune.
The underlying event that comes from the soft interactions of the proton constituents is also simulated at this stage, with the parton distribution functions again coming from phenomenological models~\cite{Lai:2010vv}.
These are also typically covered by the above-mentioned generators.
The decays of heavy flavor hadrons (those containing bottom and charm quarks) are also simulated at this stage using dedicated programs like EvtGen~\cite{Lange:2001uf}.

The other simultaneous $pp$ collisions in the event (pile-up) are not simulated but rather overlay minimum bias (i.e., minimal triggering requirements) events on top of the simulated ones, according to the expected number of interactions in the event.

The above-mentioned steps constitute what is known as the ``truth'' event.
The observables related to the particles at this stage (in particular, the four-momentum) are not accessible in real life, because they have to be observed in the detector (Section~\ref{sec:ATLAS:ATLAS}).
The simulated events are passed through a full simulation of the ATLAS detector modeled in \GEANT\cite{Agostinelli:2002hh}.
This simulates both the particle interactions with the material and the resulting digitization of the signals.
This is considered to be the ``reconstructed'' event, which is in the exact same format that real data would be observed.
The reconstructed event is passed through various post-processing stages (Section~\ref{sec:ATLAS:objects}), the overall goal of which is to attempt to construct the ``truth'' event that would have given rise to the ``reconstructed'' (nonsimulated) event as well as possible.

The majority of ATLAS computing resources are dedicated to running simulations.
Figure~\ref{fig:ATLAS:simulation} shows the fraction of disk space and CPU time projected to be needed in 2028 for the various ATLAS computing activities.
For the disk space, about 75\% of the roughly 1500 petabytes needed by ATLAS by 2028~\cite{computingandsoftware} will be dedicated to simulations (``MC'' in the Figure).
Similarly, for the CPU time, about 75\% of the roughly 20 MHS06\footnote{A HS06 is a CPU benchmark tailored for high energy physics typical use cases~\cite{hepix}. Typical high-performance CPUs are equivalent to 500-1000 HS06. A MHS06 is a mega-HS06, or 1 million HS06.} needed by ATLAS by 2028~\cite{computingandsoftware} will be dedicated to simulations (``MC'' and ``EvGen'' in the Figure).

\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{disk2028_baseline}.pdf}}
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{cpu2028}.pdf}}
  \caption{Projected computing resources needed by ATLAS in 2028. (a) Disk space. (b) CPU resources.Figures sourced from~\cite{computingandsoftware}.}
  \label{fig:ATLAS:simulation}
\end{figure}


\section{Trigger}
\label{sec:ATLAS:trigger}
The trigger system lies somewhere between the hardware and software systems\footnote{This Section is sourced mainly from~\cite{TRIG-2016-01}, with additional more recent information from~\cite{Martinez:2016udm} and~\cite{ATLAS-TRIG-2019-04-001} (which is not yet public at the time of writing).}.
Because of the enormous rate of collisions provided by the LHC ($40$ MHz), it is infeasible to store every event.
Every event takes up $\mathcal{O}($MB$)$~\cite{Buckley:2014150} on disk, so storing every event would be equivalent to $\mathcal{O}(100$ TB$)$ per second rate of writing, or about $10^6$ PB for a full year's worth of data-taking\footnote{As a piece of trivia, this amount of data is called a \textit{zetta}byte, or ZB\cite{si-brochure}.}.
Moreover, as discussed in Section~\ref{sec:LHC:luminosity} it is not desirable to store every event, because the vast majority of events are ``uninteresting'' and one of the main Physics goals of the LHC and ATLAS is to observe or discover extremely rare processes.
%This paucity of ``interesting'' physics motivates the choice to set up multiple (in 2018, about $60$~\cite{luminositypublic}) proton-proton interactions per bunch crossing, or \textit{pile-up} - in order to increase the probability of a such an interaction to occur in any given bunch crossing.

Because of the reasons outlined above, ATLAS institutes a \textit{trigger} system~\cite{TRIG-2016-01} to decide whether or not to store each event, based on whether the reconstructed objects and topology in the event are indicative of some particular physics process.
The reconstruction algorithms in use in ATLAS are distinguished by whether they are \textit{online}, meaning they are in use in the trigger and are subject to the relevant time and space constraints, or if they are \textit{offline}, meaning they are used on events passing the trigger and can be optimized for performance using the full detector information and much looser time constraints\footnote{Subject to the CPU resources available to the offline analyzer.}.

The trigger consists of two major subsystems.
First, the hardware-based \textit{Level 1} trigger, or \textit{L1}, reduces the rate of events from $40$ MHz to about $100$ kHz based on fast but approximate algorithms.
Then, the software-based \textit{high-level trigger}, or \textit{HLT}, uses offline-like algorithms (Section~\ref{sec:ATLAS:objects}) to further reduce the rate to about $1$ kHz.
The rates of each of these two trigger levels, along with some of the main individual triggers, can be seen in Figure~\ref{fig:ATLAS:trigger}.

\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{TrigOpsPublicWinter2019_L1_single-item_rates_logscale_ATLASStyle_359872}.pdf}}
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{TrigOpsPublicWinter2019_HLT_group_rates_ATLASStyle_359872}.pdf}}
  \caption{Trigger rates for (a) L1 and (b) HLT or a typical run in September 2018. The overall rate (black dashed lines) is broken down into some main individual triggers. Figures sourced from~\cite{triggerpublic}.}
  \label{fig:ATLAS:trigger}
\end{figure}

The trigger broadly works by identifying detector objects that are likely to correspond to individual particles and thus indicate a hard-scatter $pp$ interaction.
The rate of the trigger is controlled by specifying some energy threshold the detector object has to meet in order to be passed to the next trigger in the case of L1 or written to disk in the case of HLT.
The collection of trigger object definitions and corresponding energy thresholds is called the \textit{trigger menu}, and is set every year of data taking (with some minor changes between periods in the same year)~\cite{ATL-DAQ-PUB-2016-001,ATL-DAQ-PUB-2017-001,ATL-DAQ-PUB-2018-002,ATL-DAQ-PUB-2019-001} according to the instantaneous luminosity and pile-up conditions and Physics objectives.

Trigger considerations are vital for any analysis, because if the physics being targeted by an analysis does not pass any trigger then the data are simply not stored and the analysis cannot be done.
For example, the low-mass extension of the search in Chapter~\ref{ch:HBSM} discussed in Appendix~\ref{ch:HBSM_lowmass_app} spends considerable effort on finding a suitable trigger for the new signature in that search; luckily, there is an existing trigger (intended for other uses) which does have efficiency on those new signals.
However, there are a few different kinds of triggers on the trigger menu which can enable analyses in light of the trigger limitations other than the \textit{primary} triggers, which store the entire event every time the trigger is fired.
Triggers can be \textit{prescaled}, meaning there is some probability $<1$ that the trigger fires, which allows for lower energy thresholds - these can be used for background or efficiency studies or simply for monitoring the beam, detector, or trigger systems.
Also, there are situations where the energy threshold is lowered but only small fraction of the total information in the event is stored - these can be used for detector calibration or to directly conduct \textit{trigger-level} analyses (e.g.~\cite{Aaboud:2018fzt}) to be sensitive to kinematic regions that would otherwise be inaccessible with the primary triggers.

There are also special runs with entirely different trigger menus than the regular physics menu.
For example, a \textit{minimum-bias} trigger is run as the only item on the trigger menu for a short period of time (i.e., trigger on every event with some minimum quality controls with some high prescale).
These minimum-bias events are used, among other things, for measuring online trigger efficiencies offline and for overlaying pile-up events on top of simulations (Section~\ref{sec:ATLAS:simulation}).

\subsection{Level 1 Trigger}
\label{sec:ATLAS:L1}

The L1 trigger has two major subsystems, one based on observations in the calorimeter (L1Calo) and one based on observations in the muon system (L1Muon).
There is also a topological trigger (L1Topo) which combines information from the two subsystems to make trigger decisions on a whole-event basis.
In particular, there is not enough time at L1 to reconstruct tracks from hits in the tracker.

L1Calo sets up $0.1\times0.1$ in $\Delta\eta \times \Delta\phi$ trigger towers in the calorimeter and forms \textit{regions of interest}, or \textit{RoIs}, to identify physics objects.
The electron/photon and tau triggers use $2\times2$ groupings of trigger towers in the EM calorimeter that are local maxima in energy, with additional isolation requirements in the surrounding towers and in the hadronic calorimeter.
The isolation requirements are intended to distinguish between isolated photons and collimated $\pi^0\rightarrow \gamma\gamma$ decays.
The jet triggers use $4\times4$ and $8\times8$ groupings of trigger towers in the EM and hadronic calorimeters with the central $2\times2$ grouping a local maximum in energy.
A schematic of the L1Calo trigger towers and RoIs can be seen in Figure~\ref{fig:ATLAS:L1Calo}.

\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{L1Calo}.pdf}}
  \caption{Schematic view of the trigger towers used as input to the L1Calo trigger algorithms. Figure sourced from~\cite{TRIG-2016-01}.}
  \label{fig:ATLAS:L1Calo}
\end{figure}

The L1Muon system uses the information from the RPCs and TGCs (Section~\ref{sec:ATLAS:MS}) to identify muon candidates in time to pass on to the HLT.

There is finally an L1Topo system which combines information from L1Calo and L1Muon to evaluate event-wide information, in particular the \textit{missing transverse energy} or \Etmiss{}, which corresponds to inbalances of momentum in the transverse plane (Section~\ref{sec:ATLAS:met}).

\subsection{High Level Trigger}
\label{sec:ATLAS:HLT}

As mentioned above, the HLT uses offline-like reconstruction algorithms to target a wide range of more specific physics goals than the L1.
In particular, tracking information from hits in the tracker, finer-granularity information in the calorimeter, and more precise measurements in the MS allow more precise reconstruction than is available at L1.

The tracking algorithms first run a fast tracking algorithm with rough pattern recognition\footnote{Not to be confused with the similarly-named but ultimately shuttered \textit{Fast TracKer}~\cite{Shochet:1552953}, or FTK, which was a project to implement tracking directly into the hardware in order to speed up performance.} within RoIs identified at L1.
These fast tracks are then used as seeds for an offline-like (Section~\ref{sec:ATLAS:tracks}) track reconstruction algorithm.

Calorimeter reconstruction algorithms are used to identify electrons, photons, taus, and jet candidates and finally \etmiss{} global reconstruction.
The first step is to construct clusters of energy in the calorimeter based on RoI seeds from L1, for which there are two different algorithms.

The electron and photon reconstruction algorithms use a sliding-window approach by finding a window of size $0.075 \times 0.175$ in $\Delta\eta \times \Delta\phi$ that is a local maximum of energy in projective towers.
The shower shape is then found by determining layer-by-layer in the calorimeter the center of energy of the cells behind the sliding window and summing up a fixed size window around that.
This shower shape is then used to positively identify electrons and photons, e.g. using the ratio of energy deposited in the hadronic calorimeter to the electromagnetic calorimeter.

The tau, jet, and \etmiss{} reconstruction algorithms use a global topo-clustering algorithm~\cite{Aad:2016upy} very similar to the offline reconstruction algorithm (Section~\ref{sec:ATLAS:clusters}), which are built up iteratively from high-energy cell seeds.
Jets are then reconstructed using the \antikt{} algorithm with $R=0.4$ or $R=1.0$ (Section~\ref{sec:jets:def}) using these clusters as seeds.
The jets are calibrated in a similar manner to the offline procedure (Section~\ref{sec:ATLAS:jet_calibration}).
Small-$R$ jets are calibrated in particular incorporating a pile-up subtraction step, an overall MC-based \pt{} correction (the exploration of which is the subject of Chapter~\ref{ch:NI}), and the global sequential calibration (GSC) (the improvement of which is the subject of Chapter~\ref{ch:GenNI}).
Large-$R$ jets in 2017 and 2018 are trimmed and have a mass cut, but do not use any other jet substructure techniques (Section~\ref{sec:jets:tagging}) for identifying boosted massive objects.

Tau identification uses similar principles to the offline selection (Section~\ref{sec:ATLAS:EM}), looking for small-$R$ jets with narrow calorimeter energy deposits and low numbers of associated tracks.

Jets originating from $b$-quarks can also be tagged (Section~\ref{sec:ATLAS:btagging}), by identifying secondary vertices off the beamline that are characteristic of such jets.
In order to judge if a secondary vertex is significantly far enough from the beamline, it is essential to have a precise measurement of the \textit{beamspot}, which is the 3-dimensional ellipsoidal region in which $pp$ collisions can occur\footnote{With roughly a multivariate Gaussian probability distribution.}~\cite{ATLAS-CONF-2010-027}.
The Author has contributed to a project intended to more precisely measure the online beamspot using Bayesian inference~\cite{beamspot}, which could improve the $b$-tagging at HLT.

As jets with no such tagging are the most generic detector objects formed in ATLAS, the rate of jet production is very high - e.g., the cross section of producing a jet with $\Et>100$ GeV is at least a factor of $10$ higher than any other ``interesting'' electroweak process (Figure~\ref{fig:ATLAS:xs}).
Therefore, the energy thresholds for jets in the trigger are the highest of any other object, and analyses using only jet triggers are therefore limited to very high energies.
For example, the search in Chapter~\ref{ch:CWoLa} uses a single jet trigger, corresponding to offline $\pt>500$ GeV; this limits the minimum dijet invariant mass $m_{JJ}>1.1$ TeV just to meet the trigger requirements.
Because of these high requirements, there have been concerted efforts for trigger-level analyses using lower \pt{} jets~\cite{Aaboud:2018fzt}.

There are a variety of different trigger algorithms for \etmiss{}, but the most basic algorithm sums up the $\vec{\pt}$ (including the direction in the transverse plane) of all cells in the calorimeter to find any imbalance.
Other algorithms build on this by adding pile-up corrections or jet-based or topo-cluster-based associations.

The muon reconstruction algorithm first runs a fast algorithm which then seeds RoIs to be used in a precision step similar to the offline algorithm (Section~\ref{sec:ATLAS:muons}).
The fast algorithm matches the RoIs identified at L1Muon to data from the MDT chambers (Section~\ref{sec:ATLAS:MS}) to form tracks in the MS.
These tracks are then back-extrapolated back to the interaction point and combined with tracks in the inner detector.
The precision step basically repeats this process with a refined track-finding and \pt{} measurement, with an additional step of extrapolating tracks from the inner detector to hits in the MS in case the back-extrapolation fails.
Dimuon triggers (with dimuon mass requirements) are also used for tagging $b$-hadrons, e.g. $\Upsilon(1S)\rightarrow \mu\mu$ and intermediate decays involving $J/\psi\rightarrow \mu\mu$.

Finally, there are dedicated triggers with low rates for exotic signatures, e.g. long-lived particles~\cite{Aad:2013txa}.

\section{Object Reconstruction}
\label{sec:ATLAS:objects}
Events passing the triggers (Section~\ref{sec:ATLAS:trigger}) are written to disk and the detector observables are \textit{reconstructed} with algorithms into objects intended to correspond to particular physics particles.
Tracks derived from hits in the inner detector (Section~\ref{sec:ATLAS:tracks}) and topological clusters of cells in the calorimeter (Section~\ref{sec:ATLAS:clusters}) are used to build up objects corresponding to photons and electrons (Section~\ref{sec:ATLAS:EM}) and jets (Section~\ref{sec:ATLAS:jets}).
Muons are also reconstructed using inormation from the MS (Section~\ref{sec:ATLAS:muons}).
Finally, these objects are combined together to estimate the missing transverse energy (Section~\ref{sec:ATLAS:met}).

\subsection{Tracks}
\label{sec:ATLAS:tracks}
Tracks, corresponding to the paths of charged particles in the inner detector, are reconstructed from hits in the various layers of the tracker~\cite{ATL-PHYS-PUB-2015-018,Aaboud:2017all}.
An example event display showing hits in the inner detector and the tracks formed from those hits can be seen in Figure~\ref{fig:ATLAS:tracks}.
\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{JiveXML_264034_11475271_2}.png}}
  \caption{Event display showing hits in the inner detector and the tracks formed from those hits. Figure source~\cite{Collaboration:2014666}.}
  \label{fig:ATLAS:tracks}
\end{figure}

Hits in the pixel (including the IBL) and SCT layers are clustered in order to account for a single particle passing through multiple adjacent pixels or multiple particles passing through the same or adjacent pixels; these clusters are then abstracted as three-dimensional \textit{space-points}.
Track seeds are then formed from sets of three space-points.
A combinatorial Kalman filter~\cite{Fruhwirth:1987fm} is then used to incorporate additional space-points from other layers of the pixel and SCT detectors which are compatible with the particle trajectory into a track candidate.
An ambiguity solver is then used to identify clusters with track candidates, using a neural network to identify and predict merged clusters resulting from multiple tracks~\cite{Aad:2014yva}.
The clusters are identified with the track candidates in order of quality of the track, considering criteria like the number of holes (missing clusters) across the layers and $\chi^2$ of the track fit.
Track candidates are also rejected if they do not meet basic requirements like $\pt>400$ MeV (in which case the particle's trajectory would curve so much in the magnetic field it would never leave the inner detector).
Finally, a track fit is performed using all available information (in particular including information from the TRTs) to determine the track momentum (measured directly as the charge to momentum ratio $q/p$) and direction, transverse impact parameter off the beamline $d_0$ (i.e., the closest approach of the track trajectory to the beamline), and longitudinal displacement along the beamline $z_0$ (at the point of closest approach).
The tracking reconstruction is particularly difficult within the dense environments of jets, where there can be large numbers of tracks in a small region.

The resolution of track $q/p$, $d_0$, and $z_0$ as a function of $\pt$ can be seen in Figure~\ref{fig:ATLAS:track_resolution}.
\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{track_p_resolution}.png}}\\
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{track_d0_resolution}.png}}
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{track_z0_resolution}.png}}
  \caption{Track resolution of (a) $q/p$, (b) $d_0$, and (c) $z_0$ as a function of $\pt$ in cosmic-ray data. The measurements are compared among tracks measured with silicon-only components, full inner detector, and full inner detector in simulation. Figures sourced from~\cite{Aad:2010bx}.}
  \label{fig:ATLAS:track_resolution}
\end{figure}
The momentum resolution is quite good, around $1\% \oplus 0.05\% * \pt/\text{GeV}$ - the momentum resolution gets worse at high momentum as the track has less curvature in the magnetic field.
The $d_0$ resolution is important for constructing secondary vertices, which is essential for $b$-tagging (Section~\ref{sec:ATLAS:btagging}).
The $z_0$ resolution is important for constructing primary vertices which correspond to interactions along the beamline.

\subsubsection{Primary Vertices}
Primary vertices~\cite{Aaboud:2016rmg,ATL-PHYS-PUB-2015-026,Meloni:2016sag} are collections of tracks emanating from a single point indicating a $pp$ interaction at that point.
The average number of interactions per bunch crossing $\mu$ is $>1$, which is directly related to the instantaneous luminosity; $\mu>1$ is the cause of pile-up interactions in the event other than the \textit{hard-scatter} interaction or (roughly) the one that caused the event to pass the trigger.
In the ideal case the expected number of reconstructed primary vertices would scale linearly with $\mu$; however, the dominant cause of nonlinearity is vertex merging due to the high density of interactions in the luminous region.
There can also be vertex splitting, vertex fakes, and inefficiencies in vertex reconstruction.

The vertex reconstruction algorithm proceeds iteratively - after choosing a vertex seed, the algorithm alternates between finding tracks compatible with coming that vertex (subject to their $z_0$ and $d_0$ resolutions) and re-fitting the vertex position with the associated tracks until some stopping condition is met (vertices must have at least $2$ associated tracks).
The found vertex and its associated tracks are then removed from consideration and the process is repeated until all tracks are associated to a vertex or no more vertices can be found.
The output of this algorithm is a set of vertex positions and the covariance matrix of their resolutions.

The efficiency of vertex reconstruction as a function of number of associated tracks can be seen in Figure~\ref{fig:ATLAS:vertexa}.
\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{ATLAS_vertex_efficiency}.png}\label{fig:ATLAS:vertexa}}
  %\subfloat[]{\includegraphics[width=0.33\textwidth]{figures/{ATLAS_vertex_x_resolution}.png}\label{fig:ATLAS:vertexb}}
  %\subfloat[]{\includegraphics[width=0.33\textwidth]{figures/{ATLAS_vertex_y_resolution}.png}\label{fig:ATLAS:vertexc}}
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{ATLAS_vertex_z_resolution}.png}\label{fig:ATLAS:vertexd}}
  \caption{(a) Efficiency of vertex reconstruction as a function of number of associated tracks. Also, resolution of vertices in (b) $x$, (c) $y$, and (d) $z$. Measured in low-$\mu$ ($\mu\sim 0.01$) data and in MC. Figures sourced from~\cite{ATL-PHYS-PUB-2015-026}.}
  \label{fig:ATLAS:vertex}
\end{figure}
The vertex reconstruction efficiency is basically $1$ when the vertex has $5$ or more associated tracks.
Figure~\ref{fig:ATLAS:vertexd} shows the $z$ resolution of the primary vertices.
The $z$ resolution is important for distinguishing between different interactions along the beamline.

The primary vertex with the highest $\sum \pt^2$ over the associated tracks is designated as the \textit{hard-scatter} vertex (and the associated tracks are considered to be the hard-scatter tracks), as it corresponds to the hardest interaction in the event and therefore the most likely to be ``interesting''.
The remaining vertices and their associated tracks are labeled \textit{pile-up}.
For most reconstruction algorithms considered below, if they use tracks, only the hard-scatter tracks are considered.
However, the pile-up tracks are used for the removal of calorimeter jets originating from pile-up interactions, as the calorimeter by itself does not have nearly good enough angular information to distinguish between energy clusters due to hard-scatter and pile-up interactions.
Also, tracks with large impact parameters, but within some small distance of the hard-scatter primary vertex, are used to reconstruct secondary vertices for $b$-tagging.

\subsection{Clusters}
\label{sec:ATLAS:clusters}
Energy deposits in calorimeter cells are combined together using a topological clustering algorithm~\cite{Aad:2016upy}, which are then used as seeds in jet-finding.
The expected amount of noise in a given cell $\sigma_\text{noise,cell}$ is estimated in order to define an energy significance $\zeta_\text{cell} = E_\text{cell}/\sigma_\text{noise,cell}$.
The noise in a given cell is due both to electronic noise and to soft energy deposits from the pile-up interactions in the event; thus, the cell noise is estimated as a function of the position in the detector (layer of the calorimeter and $\eta$) and the amount of pile-up $\mu$.
This estimate can be seen in Figure~\ref{fig:ATLAS:cell_noise}.
\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{noise_tot_plot_OFLCOND-MC12-HPS-19-80-25}.png}}
  \caption{Expected total noise per cell (in simulation) as a function of position in the detector (layer and $\eta$) at $\mu=80$, corresponding to conditions similar to those in 2018 data-taking. Figure sourced from~\cite{LArCaloPublicResultsUpgrade}.}
  \label{fig:ATLAS:cell_noise}
\end{figure}

The algorithm for topological clustering proceeds as follows.
First, seeds are formed as cells with energy deposits $\zeta_\text{cell}>4$.
The cluster then grows from the seed to all neighboring cells (either adjacent if in the same layer or close in $\eta,\phi$ if in different layers) with energy deposits $\zeta_\text{cell}>2$; this growth continues iteratively until there are no more cells passing the threshold to spread to.
Finally, the cluster spread to all immediately neighboring cells with $\zeta_\text{cell}>0$\footnote{Note that the significance can be $<0$ due to pulse-shape calibrations in the cell.}.
Clusters can then be split if there are local maxima of energy within the cluster.
The result of the clustering algorithm is a set of 3-dimensional irregularly-shaped but topologically-connected clusters.
The progression of this algorithm can be seen in Figure~\ref{fig:ATLAS:topoclustering}.
\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{topoclustering_step1}.png}}
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{topoclustering_step2}.png}}\\
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{topoclustering_step3}.png}}
  \caption{Progression of the topological cell clustering algorithm in a single layer of the calorimeter. (a) Cell seeds passing $\zeta_\text{cell}>4$. (b) Growth of clusters to cells passing $\zeta_\text{cell}>2$. (c) Final clusters including cells with $\zeta_\text{cell}>0$ and cluster splitting. Figures sourced from~\cite{Aad:2016upy}.}
  \label{fig:ATLAS:topoclustering}
\end{figure}

Each cluster is assigned an ($\eta,\phi$) position based on the barycenter of the constituent cells, and a total energy $E$.
Clusters are assumed to be massless when forming their four-momentum.

The electromagnetic showers (due to electrons and photons) have a different response in the calorimeter than hadronic showers due to the different kinds of interactions with the detector at play.
Because of this, there is a desire to calibrate individual clusters to a consistent scale.
This calibration does not have to be applied - in the case that it is not applied, the clusters and the jets formed from them are considered to be at the \textit{EM scale}.
For example, the small-$R$ jets used in the search presented in Chapter~\ref{ch:HBSM} use jets at the EM scale.
However, large-$R$ jets usually do use calibrated clusters, as they often take advantage of substructure information in their constituent clusters, which benefits from the clusters being at a consistent energy scale.
The large-$R$ jets used in the search presened in Chapter~\ref{ch:CWoLa} use jets formed from these calibrated clusters.

The calibration is a \textit{local cluster weighting}, or \textit{LCW}.
First, the entire cluster is classified according to the likelihood it comes from an electromagnetic or hadronic shower based on its position in the detector and various cluster moments.
Then, each cell in the cluster is corrected as a weighted average between a hadronic correction and an electromagnetic correction according to this likelihood.
The hadronic corrections are derived from simulations of single-particle charged pions interacting with the detector, and the electromagnetic corrections are derived from simulations of single-particle neutral pions ($\rightarrow \gamma\gamma$).
The per-cell hadronic correction consists of three components: a correction for the difference between hadronic and electromagnetic sources; a correction for out-of-cluster cells; and a correction accounting for dead material in front of the calorimeters.
The per-cell electromagnetic correction consists of the second two components in the hadronic correction (out-of-cluster and dead material effects).
The entire cluster four-momentum is then recalculated using the weighted cells.
This calibration process is outlined in Figure~\ref{fig:ATLAS:LCW}.
\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.9\textwidth]{figures/{LCW_diagram}.png}}
  \caption{Steps in LCW cluster calibration scheme. After the topo-clustering formation and splitting, clusters are classified based on the likelihood of coming from a hadronic or electromagnetic shower. This likelihood is used as the weighting between a hadronic and electromagnetic correction. The correction includes a correction for differences between hadronic and electromagnetic showers (for which the electromagnetic correction is $1$), out-of-cluster effects, and dead material. Figure sourced from~\cite{Aad:2016upy}.}
  \label{fig:ATLAS:LCW}
\end{figure}

\subsection{Photons and Electrons}
\label{sec:ATLAS:EM}
The reconstruction of photons and of electrons~\cite{Aad:2019tso} are quite similar to each other.
This is due to the fact that, as mentioned in Section~\ref{sec:ATLAS:calorimeter}, electrons and positrons radiate bremsstrahlung photons when interacting with the detector, and photons in turn often \textit{convert} into electron/positron pairs - so the electromagnetic showers produced by electrons and photons in the calorimeter are very similar.
A schematic of an electron passing through the detector can be seen in Figure~\ref{fig:ATLAS:ATLAS_trackpath}, showing in addition a radiated bremmstrahlung photon\footnote{Without the dashed line, this is also just a generic path of a charged particle through the detector.}.
\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{ATLAS_trackpath}.png}}
  \caption{Schematic of the path of an electron through the ATLAS detector. The red solid line can be reconstructed as a track in the inner detector, and the red dashed line shows the path of a radiated bremsstrahlung photon due to material interactions in the inner detector. Figure sourced from~\cite{Aaboud:2019ynx}.}
  \label{fig:ATLAS:ATLAS_trackpath}
\end{figure}

Electrons and photons are distinguished mostly by tracking information - electrons have a matched track emanating from the primary vertex, while photons either have no associated tracks (if unconverted) or a set of paired tracks emanating from a secondary conversion vertex (if converted).

The reconstruction begins with topoclusters (Section~\ref{sec:ATLAS:clusters}), with a preselection that $>50\%$ of the topocluster energy is in the electromagnetic calorimeter.
Tracks are then extrapolated from the tracker into the calorimeter and matched to topoclusters in $\eta,\phi$.
As mentioned above, matched tracks are then determined to come from the primary vertex or combined with other nearby tracks to find secondary vertices indicating a converted photon.
The topoclusters are then used as seeds to expand into \textit{superclusters} intended to capture the full electromagnetic shower.
Topoclusters with $\ET>1$ GeV and a matched track, or topoclusters with $\ET>1.5$ GeV and no requirement on matched tracks are used as superclusters seeds.
In order of seed $\ET$, the seeds are expanded to all neighboring topoclusters within $\Delta\eta \times \Delta\phi = 0.075\times 0.125$.
If there is a matched primary vertex track to the seed then any other neighboring clusters matched to the same track are also added; and if there is a matched conversion vertex to the seed then any other neighboring clusters with tracks matching the conversion vertex are added.
Finally, tracks are refit to the superclusters, and they are labeled unambiguous electrons if there is a matched primary vertex track, unambiguous photons if there are no matched tracks or a set of matched tracks forming a conversion vertex, or ambiguous if both are true.

A variety of features related to the shower shape are used for electron and photon identification and discrimination from hadronic showers.
For example, the fraction of energy of the supercluster in the electromagnetic calorimeter is used as a discriminatory variable; the distribution of this feature and discrimination power in electrons and in pile-up clusters (mostly hadronic) can be seen in Figure~\ref{fig:ATLAS:EMfrac}.
\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{EMfrac_electrons_pileup_dist}.png}}
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{EMfrac_electrons_pileup_ROC}.png}}
  \caption{Fraction of supercluster energy in electromagnetic calorimeter, compared between true electrons and pile-up, in simulation. (a) Distribution; (b) Efficiency of cut. Figure sourced from~\cite{Aad:2019tso}.}
  \label{fig:ATLAS:EMfrac}
\end{figure}
These features (formed separately for electrons and photons) are combined together into a multivariate likelihood in bins of \ET{} and $\eta$ to be used in the final identification.
More information about the electron identification can be found in~\cite{Aaboud:2019ynx}.
The shower shape variables are particularly useful for distinguishing between prompt photons and neutral pions decaying to collimated $\gamma\gamma$.
Both photons and electrons have working points at \texttt{Loose}, \texttt{Medium}, or \texttt{Tight}, which can be chosen at the analysis level based on the signal efficiency - background rejection tradeoff.

In addition, a set of isolation variables are defined based on calorimeter and tracking activity outside the supercluster but inside a cone of $\Delta R = 0.2$.
The isolation is intended to discriminate between prompt electrons and electrons from heavy flavor decays or light hadrons misidentified as electrons, and further between prompt photons and neutral pions decaying to collimbated $\gamma\gamma$.
Isolation working points are defined at \texttt{Loose}, \texttt{Tight}, and \texttt{CaloOnly} which uses only calorimeter information.

The efficiency of the identification and isolation working points are evaluated in data using high-fidelity $Z\rightarrow ee$ and $J/\psi \rightarrow ee$ events for electrons~\cite{ATLAS-CONF-2016-024} and $Z\rightarrow ee\gamma$ and $Z\rightarrow \mu\mu\gamma$ events for photons~\cite{PERF-2013-04,Aaboud:2018yqu}.
The differences between the simulation and data are corrected, and the uncertainties on these corrections are applied as systematic uncertainties on simulated events.

The energy of electrons and photons are calibrated~\cite{Aaboud:2018ugz,PERF-2013-05} to account for energy lost in the material upstream of the calorimeter, for energy deposited in the cells neighbouring the cluster, and for energy lost beyond the electromagnetic calorimeter.
The corrections are first derived in simulation using a boosted decision tree (BDT) incorporating multiple shower shape variables in bins of $\eta$ and $Et$.
The photon and energy calibrations do not use numerical inversion (Chapter~\ref{ch:NI}), meaning the calibrations can be biased by the prior used to calibrate.
However, as the energy resolution is small ($\mathcal{O}(5\%)$ for electrons and photons with $5<Et<10$ GeV, and better for higher energies), this effect is not as important as it would be for jets, which have much worse resolutions (Section~\ref{sec:ATLAS:jet_calibration}).

The calibrated energies are evaluated in data using the above-mentioned high-fidelity samples.
A data-simulation, or \textit{in situ}, correction is applied based on any observed differences.
The uncertainties on these corrections are applied as systematic uncertainties on simulated events.

\subsection{Jets}
\label{sec:ATLAS:jets}
There are a variety of jet collections in use in ATLAS.
The most common are those formed using topoclusters (Section~\ref{sec:ATLAS:clusters}) as seeds, with $R=0.4$ (small-$R$ jets) using topoclusters at the EM scale and $R=1.0$ (large-$R$ jets) using topoclusters at the LCW scale.
Large-$R$ jets are typically trimmed as described in Section~\ref{sec:jets:grooming}.

Jets can also be formed using particle flow objects~\cite{Aaboud:2017aca} which combine information from the calorimeter and tracker; Track-CaloClusters~\cite{ATL-PHYS-PUB-2017-015} which similarly combine information from the calorimeter and tracker; or reclustering from smaller-radius jets~\cite{Nachman:2014kla}.
Topoclusters themselves can also be corrected other than the LCW calibration - the Author has contributed to a project~\cite{ATLAS-CONF-2017-065} with the idea of subtracting pile-up from topoclusters before running the jet algorithm.

However, even if the seeds are in principle themselves corrected or calibrated, there is still a need to calibrate the jets formed from those seeds to further account for pile-up, out-of-cone, and nonlinearity effects.

\subsubsection{Jet Calibration}
\label{sec:ATLAS:jet_calibration}
The jet energy and momentum calibration procedure~\cite{PERF-2016-04,Aad:2011he,Aaboud:2018kfi} is achieved through a series of steps.

The steps for the calibration of small-$R$ jets are shown in Figure~\ref{fig:ATLAS:smallR_calibration}.
\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.8\textwidth]{figures/{smallR_calibration}.png}}
  \caption{Steps in the small-$R$ jet calibration. Figure sourced from~\cite{PERF-2016-04}.}
  \label{fig:ATLAS:smallR_calibration}
\end{figure}
Following jet reconstruction from the calorimeter cell-clusters, there is a minor correction to the jet direction to point to the hard-scatter primary vertex rather than the nominal detector center.
The impact of pile-up is corrected for using a jet area-based~\cite{Cacciari:2007fd,Cacciari:2008gn} approach followed by a residual correction sensitive to both in-time and out-of-time pile-up~\cite{Aad:2015ina}.
Following the pile-up correction, an absolute \pt{} scale correction is derived in simulation, which also corrects the jet direction.
There is then a sequence of corrections, called the \textit{global sequential calibration} or \textit{GSC}, to further correct the residual dependence of the jet \pT{} on various jet quantities using information from the tracker, calorimeter, and MS.
The final step of the jet calibration procedure applied only to data is an in-situ correction that accounts for the residual differences between data and simulation.


The steps for the calibration of large-$R$ jets are shown in Figure~\ref{fig:ATLAS:largeR_calibration}.
\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.8\textwidth]{figures/{largeR_calibration}.png}}
  \caption{Steps in the large-$R$ jet calibration. Figure sourced from~\cite{Aaboud:2018kfi}.}
  \label{fig:ATLAS:largeR_calibration}
\end{figure}
After jet reconstruction with LCW topoclusters and grooming with trimming, the jet energy, mass, and $\eta$ are corrected using an absolute MC-based correction.
There is finally an in-situ correction applied only to data to correct for differences in the energy scale between data and simulation.

Each of the above steps, other than the pile-up correction for small-$R$ jets, operate under the principle of \textit{numerical inversion} (Chapter~\ref{ch:NI}) in order to be independent of the underlying prior distribution of the reference energy or momentum.

In the simulation-based corrections, the reference object is ``truth'' jets formed from detector-stable simulated particles ($c\tau > 10$ mm) other than muons and neutrinos, absent the detector simulation (Section~\ref{sec:ATLAS:simulation}).
Reconstructed jets are geometrically matched to truth jets with $\Delta R = \sqrt{\Delta\eta^2+\Delta\phi^2}$.
Truth jets are matched to truth partons using ghost association~\cite{Cacciari:2008gn}; the type of the highest energy parton matched to a truth jet is used as the label.

In the in situ corrections, the reference object is some better measured object in the event, taking advantage of conservation of momentum in the entire event.
For lower \pt{} jets, the in situ corrections use $Z\rightarrow ee/\mu\mu$ or $\gamma$ as the reference object to balance the momentum.
For higher \pt{} jets, systems of multiple lower \pt{} small-$R$ jets are used to balance the momentum.

The first step of the small-$R$ jet correction is simply a geometric correction to account for the fact that $\eta$ is measured as an angle from the center of the detector, but jet angle should instead be measured from the interaction point or the hard-scatter primary vertex.

The pile-up correction for small-$R$ jets~\cite{Cacciari:2007fd} first estimates the average extra \pt{} density due to pile-up in the event $\rho$, and the catchment area of the jet $A$; then $\rho A$ is subtracted from the jet \pt{} as the expectation of pile-up contribution in the jet.
The areas of jets~\cite{Cacciari:2008gn} are estimated using \textit{ghost association}.
The whole event is blanketed uniformly in $\eta,\phi$ with zero-momentum \textit{ghosts}, and the jet finding process is repeated.
The set of output jets is not affected by the presence of ghosts, since the algorithm is IRC safe (Section~\ref{sec:jets:def}), but the set of ghosts associated with each jet gives a definition of the jet area.
$\rho$ is calculated by rerunning the jet finding using the $\kt$ algorithm with $R=0.4$ (most of which will be due to pile-up interactions), calculating the jet momentum density $\pt/A$, and taking the median over all jets.
The distribution of $\rho$ at different values of the number of primary vertices (NPV), an estimate of the pile-up contribution in the event, can be seen in Figure~\ref{fig:ATLAS:jet_calibration_pileup}.
It can be seen that the peak of the $\rho$ distribution increases roughly linearly with NPV.

The Author has been involved in a project~\cite{ATLAS-CONF-2017-065} to subtract pile-up from topoclusters in the first place rather than from the entire jet.
In this project, $\rho$ is calculated in the same way, but the area of each topocluster is estimated using a Voronoi tessellation~\cite{LejeuneDirichlet1850,Voronoi1908,10.1145/10515.10549} of the calorimeter in $\eta,\phi$ with the topoclusters as seeds.

After the $\rho A$ correction, there is still some residual dependence on NPV and $\mu$, particularly at high $\eta$.
The dependence of the jet \pt{} on these two parameters is estimated with a linear fit and subtracted; the effect of this correction can be seen in Figure~\ref{fig:ATLAS:jet_calibration_pileup}.

\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{jet_calibration_rho}.png}}\\
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{jet_calibration_residNPV}.png}}
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{jet_calibration_residmu}.png}}
  \caption{(a) Distribution of pile-up density $\rho$ for different values of NPV at fixed $\mu$. (b) Effect of residual correction for NPV. (c) Effect of residual correction for $\mu$. Figures sourced from~\cite{Aaboud:2018kfi}.}
  \label{fig:ATLAS:jet_calibration_pileup}
\end{figure}

Following the pile-up correction, a correction is derived in simulation for the ratio between the overall reconstructed jet energy and the truth jet energy.
In bins of $\eta$, the mode of the $E^\text{reco}/E^\text{true}$ distribution (the energy \textit{response}) in each $E^\text{true}$ bin is found by fitting a Gaussian to the central peak of the distribution and taking the mean of the fitted Gaussian.
As mentioned above, this process proceeds via numerical inversion (Chapter~\ref{ch:NI}), by inverting this response function and applying it to the reconstructed energies.
The dependence of the energy response on $E^\text{true}$ and $\eta$ can be seen in Figure~\ref{fig:ATLAS:jet_calibration_NI}.

Following the absolute scale correction, a sequence of residual corrections on various features are applied to correct the jet \pt{}.
The sequence of corrections, called the \textit{global sequential calibration} or \textit{GSC}, corrects the dependence of \pt{} on each feature sequentially; since the dependence of the \pt{} response changes with \pt{}, these steps also proceed via numerical inversion.
The features include those sensitive to the provenance of the jet (quark- or gluon-initiated) like the number of tracks in the jet and the width of tracks in the jet (the average \pt-weighted distance of tracks from the jet axis); removing this dependence makes the response more similar for quark and gluon jets and reduces the uncertainty due to jet fragmentation modeling for a given jet type.
There are also two features to account for the differences between electromagnetic and hadronic showers (based on the fraction of jet energy in various layers of the electromagnetic and hadronic calorimeters), and finally there is a feature to account for hadronic showers that punch through to the MS.
The dependence of the \pt{} on some of the features that go into the GSC can be seen in Figure~\ref{fig:ATLAS:jet_calibration_NI}.

Chapter~\ref{ch:GenNI} proposes a new method of accounting for the dependence of the jet \pt{} on each of these auxiliary variables by deriving the correction simultaneously rather than sequentially.
The new method is enabled by the use of neural networks to estimate the response dependence in the high-dimensional space formed by all the features, but retains the key properties of numerical inversion.

\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{jet_calibration_NI}.png}}\\
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{jet_calibration_GSC_wtrk}.png}}
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{jet_calibration_GSC_ntrk}.png}}
  \caption{Small-$R$ jet calibration. (a) Energy response in bins of $E^\text{true}$ and $\eta$ for the absolute energy scale correction. (b) Dependence of \pt{} on track width for the GSC. (c) Dependence of \pt{} on number of tracks in jet for the GSC. Figures sourced from~\cite{Aaboud:2018kfi}.}
  \label{fig:ATLAS:jet_calibration_NI}
\end{figure}

The above corrections are applied to all jets, both in simulation and data.
There is a final step which compares the ratio of the jet response to a reference object in simulation and in data, and if there is a difference then the jets in data are corrected; this is called the \textit{in situ} correction.
As mentioned above, the reference objects include well-reconstructed $Z \rightarrow ee/\mu/mu$, $\gamma$, or multijet systems, taking advantage of conservation of momentum in the event.
The correction using $Z$ as the reference object cover the low $\pt$ range $20 < \pt < 500$ GeV; the correction using $\gamma$ as the reference object cover the medium $\pt$ range $36 < \pt < 950$ GeV; and the multijet balance is used to extend the calibration up to $2$ TeV.
There is also an in situ $\eta$-intercalibration comparing jets at high $\eta$ to more central jets.
This correction also proceeds via numerical inversion.
There can be uncertainties in the value of this correction due to mismodeling of physics effects; uncertainties in the kinematics of the reference object; and uncertainties in the \pt{} balance due to the event topology.
These uncertainties are propagated at systematic uncertainties on the jet energy correction.
The total systematic uncertainty on the jet energy is around 2\%, rising up to around 5\% for low-$\pt$ jets.

The calibration of large-$R$ jets~\cite{Aaboud:2018kfi} does not include a pile-up subtraction step, as large-$R$ jets are trimmed (Section~\ref{sec:jets:grooming}) and are intended for high \pt{} massive objects for which pile-up has a small effect on the jet \pt{}.
The energy correction for large-$R$ jets in bins of $\eta$ is derived in simulation similarly to that for small-$R$ jets, again using numerical inversion.
After the energy correction the large-$R$ jet mass is also calibrated in bins of $E$ and $\eta$, again using numerical inversion to prevent being biased by the distribution of the truth mass.
The energy and mass response before calibration can be seen in Figure~\ref{fig:ATLAS:largeR_jet_calibration_response}.

\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{largeR_jet_calibration_E}.png}}
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{largeR_jet_calibration_M}.png}}
  \caption{Large-$R$ jet calibration. (a) Energy response in bins of $E^\text{true}$ and $\eta$ for the absolute energy scale correction. (b) Mass response in bins of $\pt^\text{true}$ and $\eta$ for jets with truth mass around $m_W \approx 80$ GeV. Figures sourced from~\cite{Aaboud:2018kfi}.}
  \label{fig:ATLAS:largeR_jet_calibration_response}
\end{figure}

The Author has been involved in a project~\cite{ATL-PHYS-PUB-2020-001} intended to calibrate the jet energy and mass simultaneously rather than the sequential process outlined above.
This project uses the simultaneous techniques enabled by neural networks introdued in Chapter~\ref{ch:GenNI}.

There are two linearly independent definitions of the jet mass~\cite{ATLAS-CONF-2016-035}.
The first is directly from summing together the four momenta of the constituent topoclusters (which are themselves assumed to be massless) and is called $m^\text{calo}$, or \textit{calorimeter} mass.
The second, called the \textit{track} mass or $m^\text{track}$, is measured from tracks ghost-associated to the jet.
This measurement can be more accurate than the calorimeter mass due to the improved angular resolution of tracks compared to topoclusters; however, the tracks only comprise the charged particle component of the hadronic shower, so the overal scale of the mass will be off, even as the angular distribution is relatively accurate.
To account for the absence of neutral particles in the track mass which are measured in the calorimeter, the track mass is scaled up by the ratio of the calorimeter and track \pt{}, forming the \textit{track-assisted} mass~\cite{Elder:2018mcr} or $m^\text{TA}=m^\text{track}\frac{\pt^\text{calo}}{\pt^\text{calo}}$.
Each of these two mass definitions are calibrated separately.
The \textit{combined} mass or $m^\text{comb}$ is defined as a weighted sum between $m^\text{track}$ and $m^\text{TA}$ based on the (inverse) resolution of the truth jet mass under each definition.
Since the sum of the weights is $1$, $m^\text{comb}$ is itself calibrated.

As with small-$R$ jets, an in situ correction is applied to the large-$R$ jet \pt{} in data, using as reference objects well-reconstructed $Z \rightarrow ee/\mu/mu$, $\gamma$, or multijet systems.
The correction using $Z$ as the reference object cover the low $\pt$ range $200 < \pt < 500$ GeV; the correction using $\gamma$ as the reference object cover the medium $\pt$ range $500 < \pt < 1000$ GeV; and the multijet balance is used to extend the calibration up to $2.5$ TeV.
There is also an in situ $\eta$-intercalibration comparing jets at high $\eta$ to more central jets.
The in situ correction or large-$R$ jets also proceeds via numerical inversion.
The total systematic uncertainty on the jet energy rises from around 2\% at low \pt{} to around 6\% for high-$\pt$ jets.

An in situ correction for the jet mass is also derived, though this correction cannot take advantage of \pt{} balance with a reference object.
Two methods are employed to derive this correction.
The first method compares the ratio of $m^\text{calo}$ to $m^\text{track}$ in data and simulation and provides a correction based on any differencs, and uncertainties on this correction are applied as systematic uncertainties.
The second method is called \textit{forward folding}~\cite{ATLAS-CONF-2016-008}.
Both data and simulation are subjected to an event selection targeting $t\bar{t}$ events with high fidelity.
This event selection takes advantage of events where one of the top quarks decays semileptonically ($t\rightarrow Wb \rightarrow l\nu b$) to unambiguously identify the top quark and the other decays hadronically ($t\rightarrow Wb \rightarrow qqb$), using the hadronic decay to calibrate the jet mass.
In these events a correction to the mean and resolution of the mass response is derived by matching the entire distribution seen in simulation to that seen in data (for the given choice of mass definition); this correction is taken as the in situ correction and uncertainties related to the measurement are taken as systematic uncertainties.
A jet-topology dependence and uncertainty can also be derived from forward folding by requiring the tagged $b$-jet (Section~\ref{sec:ATLAS:btagging}) to be inside (for top jets) or outside (for $W$ jets) the large-$R$ jet.
The jet mass response uncertainty depends on the jet \pt{}, but is generally less than $5\%$.

The tagging of jets based on their substructure is covered in Section~\ref{sec:jets:tagging}.
Briefly, small-$R$ jets can be tagged as originating from quarks or gluons based on the distribution of tracks within the jet (gluons are wider than quarks).
Large-$R$ jets can be tagged as fully hadronic decays originating from $W/Z$ bosons (with two hard subprongs), from top quarks (with a $W$ and a $b$-tagged subjet (Section~\ref{sec:ATLAS:btagging})), or from Higgs bosons (with two $b$-tagged subjets).
There are also decays involving leptons that can be used to tag large-$R$ jets, e.g. $Z\rightarrow ee/\mu\mu$.

The following three Sections cover jet tagging not based strictly on the angular distribution of the tracks, topoclusters, and subjets within the jet: pile-up jet tagging (Section~\ref{sec:ATLAS:JVT}), $b$-tagging (Section~\ref{sec:ATLAS:btagging}), and $\tau$-tagging (Section~\ref{sec:ATLAS:taus}).

\subsubsection{Pile-up Jet Tagging}
\label{sec:ATLAS:JVT}
While the pile-up contribution within small-$R$ jets is subtracted out in the calibration process, there can be jets with low transverse momentum that originate entirely from pile-up.
These jets can either be stochastic, meaning local fluctuations in the soft particle noise level from the sum of the other interactions in the event, or they can result from hard partons that originate from a single interaction other than the hard-scatter vertex (which presumably produced even harder objects to fire the trigger and have the highest $\sum \pt^2$.).

The dependence of the number of jets in the event on the average number of interactions per bunch crossing $\mu$ is shown in Figure~\ref{fig:ATLAS:JVT}.
It can be seen that there is a linear component of the dependence on pile-up, corresponding to hard partons from a single interaction, and a superlinear component, corresponding to stochastic fluctuations.

\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{pileup_JVT}.pdf}}
  \caption{Dependence of number of jets ($\pt>20$ GeV) per event on the average number of interactions per bunch crossing. In blue, all jets. In red, after applying a selection of JVT$>0.59$. Events from 2017 data-taking with a dimuon trigger and $81<M_{\mu\mu}<101$ GeV, correponding to an event topology of $Z+$jets. Figure sourced from~\cite{JETM-2017-009}.}
  \label{fig:ATLAS:JVT}
\end{figure}

It is desirable to remove these pile-up jets from the event, since they are unrelated to the hard-scatter topology and would otherwise be a major source of noise in event selections.
Jets are identified as \textit{pile-up jets}~\cite{Aad:2015ina,ATLAS-CONF-2014-018} (rather than \textit{hard-scatter jets}) based on tracks ghost-associated with the jet and identified as originating from the hard-scatter vertex or a pile-up vertex. 

The \textit{jet-vertex-fraction}, or \textit{JVF}, is defined as the fraction of the scalar sum of associated track \pt{} coming from the hard-scatter primary vertex.
While hard-scatter jets tend to have higher values of JVF than pile-up jets, the efficiency of a fixed cut on JVF degrades as the number of primary vertices increases.
A \textit{corrected JVF}, or \textit{corrJVF}, variable is defined to account for this effect which compares the scalar sum of the associated track \pt{} coming from the hard-scatter primary vertex to the average (as opposed to total in the case of JVF) scalar sum of track \pt{} coming from pile-up vertices.
Because of this correction, the efficiency of a fixed cut on corrJVF does not degrade with the number of primary vertices.

An additional variable, $R_{p\text{T}}$, is defined as the ratio of the scalar sum of associated track \pT{} coming from the hard-scatter primary vertex to the total \pt{} of the jet measured in the calorimeter, including the calibration with the pile-up subtraction step.
This variable accounts for false positives with corrJVF in the case that a stray soft track from the hard-scatter vertex happens to land in a pile-up jet with no associated tracks.
Hard-scatter jets tend to have a broad distribution in $R_{p\text{T}}$ depending on the charged fraction of the shower, while pile-up jets have a sharp distribution near $0$.
As $R_{p\text{T}}$ is defined in terms of only the hard-scatter vertex, it is to first order independent of the number of primary vertices.

A final variable, \textit{jet-vertex-tagger} or \textit{JVT}, is defined as a combination of these two variables.
The two-dimensional likelihood distribution in hard-scatter jets and pile-up jets is formed using simulated dijet events.
JVT is defined as the likelihood ratio between these two samples, estimated using a $k$-nearest neighbor algorithm~\cite{hoecker2007tmva} in $(\text{corrJVF},R_{p\text{T}})$ space with a Euclidean metric and $k=100$.

The curve for the rate of pile-up jets vs the efficiency on hard-scatter jets passing a selection on JVF, corrJVF, $R_{p\text{T}}$, and JVT is shown in Figure~\ref{fig:ATLAS:pileuprejection}.
It can be seen that JVT performs the best over all algorithms, and in particular that while JVT and corrJVF perform similarly at high efficiency, JVT removes some false positives at lower efficiency due to the incorporation of $R_{p\text{T}}$.

\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{pileuprejection}.pdf}}
  \caption{Fake rate (average number of pile-up jets per event) vs efficiency on hard-scatter jets as the selection on JVF, corrJVF, $R_{p\text{T}}$, and JVT is varied, in simulation. Figure sourced from~\cite{ATLAS-CONF-2014-018}.}
  \label{fig:ATLAS:pileuprejection}
\end{figure}

A typical selection for jets in use in an analysis is $\text{JVT}>0.59$, which has about a 90\% efficiency on hard-scatter jets and a $0.02$ pile-up jet rate.
This selection is applied only to jets with $\pt<60$ GeV, as the prior distribution of pile-up jets falls off rapidly with \pt{} and so a pile-up jet suppression is unnecessary (and would remove some high \pt{} hard-scatter jets).
Pile-up jet suppression is not applied to large-$R$ jets, as they are only calibrated down to $200$ \GeV{} and the rate of pile-up jets at that scale is negligible.

JVT of course relies on tracks and track-to-vertex association for pile-up identification, and so can only be applied to jets with $|\eta|<2.4$ (\textit{central jets}), the angular extent of the tracker (Section~\ref{sec:ATLAS:tracker}).
For jets outside this range, or \textit{forward jets}, a different pile-up tagger can be used~\cite{Aaboud:2017pou}.
The rate of forward pile-up jets is less than that for central pile-up jets, as can be seen in Figure~\ref{fig:ATLAS:forwardpileup}.
However, it is important to remove these forward pile-up jets for analyses that expect forward jets as part of their signal selection - for example, the search presented in Chapter~\ref{ch:HBSM} targets Higgs bosons produced in the vector-boson-fusion mode, which tends to produce forward quark-initiated jets.

In contrast to central jets, forward pile-up jets are more likely to come from hard partons from a single pile-up interaction (\textit{QCD pile-up jets}) than from stochastic fluctuations from multiple interactions (\textit{stochastic pile-up jets}), as can be seen in Figure~\ref{fig:ATLAS:forwardpileup}.

\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{forward_pileup_rate}.pdf}}
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{stochastic_pileup_rate}.pdf}}
  \caption{(a) Average number of events with at least one forward jet ($\pt>20$ GeV) as a function of the average number of interactions per bunch crossing $\mu$. (b) QCD pile-up jet fraction as a function of \pt{} for central and forward jets. Figures sourced from~\cite{Aaboud:2017pou}.}
  \label{fig:ATLAS:forwardpileup}
\end{figure}

Stochastic pile-up jets are identified using the spatial width $\gamma$ and temporal width of calorimeter towers in the catchment area of the jet, as hard-scatter jets tend to have a well-defined small core and stochastic pile-up jets tend to be diffuse.

QCD pile-up jets are identified taking advantage of the fact that the total momentum from each pile-up interaction must be conserved in the event.
Therefore, events with forward QCD pile-up jets tend to have pile-up vertices with some missing vectorial transverse momentum in their associated tracks and jets, which matches the vectorial transverse momentum present in the forward jet.
Forward QCD pile-up jets can be identified taking advantage of this event topology; this algorithm is called \textit{forward JVT} or \textit{fJVT}.

The taggers based on $\gamma$ and fJVT are combined together in a forward pile-up jet tagger called fJVT$_\gamma$, and the final tagger uses both fJVT$_\gamma$ and timing information to suppress forward stochastic and QCD pile-up jets.
The performance of these taggers can be seen in Figure~\ref{fig:ATLAS:fJVT}.
\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{fJVT_2030}.pdf}}
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{fJVT_3050}.pdf}}
  \caption{Efficiency on pile-up jets vs efficiency on hard-scatter jets for taggers based on $\gamma$+timing information, fJVT, and fJVT$_\gamma$+timing information. (a) Jets with $20<\pt<30$ GeV; (b) jets with $30<\pt<50$ GeV. Figures sourced from~\cite{Aaboud:2017pou}.}
  \label{fig:ATLAS:fJVT}
\end{figure}

The efficiencies of both JVT and the forward pile-up tagger are measured in data using $Z\rightarrow \mu\mu$+jets events and corrected for any data-simulation differences; the uncertainties on these corrections are applied as systematic uncertainties.

\subsubsection{$b$-tagging}
\label{sec:ATLAS:btagging}
Jets originating from $b$-quarks (\textit{$b$-jets}) can be identified taking advantage of the lifetime of $B$-hadrons , which is long enough for the decay to be measurably distant from the primary vertex but short enough that the decay occurs in the inner detector ($\mathcal{O(\text{mm})}$)\footnote{$b$-jets contain electrons or muons (+neutrinos) about 10\% of the time each; in those cases the leptons are used to identify the $b$-jet. This Section focuses on fully hadronic decays.}.
While the main background is jets originating from light quarks (\textit{light-flavor jets}), jets originating from $c$-quarks (\textit{$c$-jets}) also present a confounding background source, as hadrons containing $c$-quarks also tend to have a relatively long lifetime.

There are a variety of $b$-tagging algorithms~\cite{Aad:2015ydr,Aaboud:2018xwy,Aad:2019aic}, which combine low-level features related to the $B$-hadron decay into high-level multivariate taggers.
The low level features fall into a few different categories: using the impact parameter of tracks matched to the jet directly (\textit{IP2D} or \textit{IP3D})~\cite{ATL-PHYS-PUB-2017-013}; constructing secondary vertices using matched tracks (\textit{SV1})~\cite{ATL-PHYS-PUB-2017-011}; or fitting the entire $B$-hadron decay (\textit{JetFitter})~\cite{ATL-PHYS-PUB-2018-025}.
These features are combined into two high-level multivariate tagging algorithms: \textit{MV2}, which is trained using a boosted decision tree in TMVA~\cite{hoecker2007tmva}, and \textit{DL1}, which is trained using a deep feed-forward neural network in Keras~\cite{chollet2015keras}.
The distributions of the scores from these high-level taggers can be seen in Figure~\ref{fig:ATLAS:btagging}.
The background rejection rate, for light-flavor jets or for $c$-jets, as a function of the $b$-jet efficiency, for each of the above low-level and high-level taggers, can also be seen in Figure~\ref{fig:ATLAS:btagging}.
DL1 does slightly better than MV2, with a rejection rate of light-flavor jets of about $10$ at $70\%$ efficiency on $b$-jets.

\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{MV2_dist}.pdf}}
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{DL1_dist}.pdf}}\\
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{btagging_ROC}.pdf}}
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{btagging_ROC_cjets}.pdf}}
  \caption{Distribution of scores from high-level $b$-taggers for light-flavor jets, $c$-jets, and $b$-jets: (a) MV2; (b) DL1. Background rejection rate of various $b$-taggers vs efficiency on $b$-jets: (c) light-flavor jets; (d) $c$-jets. Figures sourced from~\cite{Aad:2019aic}.}
  \label{fig:ATLAS:btagging}
\end{figure}

Typically analyses will use working points at the $60\%$, $70\%$, $77\%$, or $85\%$ efficiency levels, depending on the needs of the analysis.

The efficiency of the $b$-taggers is measured in data in high-fidelity $t\bar{t}$ events, where both of the top quarks decays leptonically $t\rightarrow Wb\rightarrow l\nu b$.
Any differences between data and simulation are corrected, and any uncertainties in this correction are applied as systematic uncertainties on the taggers.

\subsubsection{Taus}
\label{sec:ATLAS:taus}
Tau leptons decay hadronically about 65\% of the time~\cite{PDG}, in which case they are reconstructed as small-$R$ jets (\textit{tau-jets})\footnote{Taus decay to electrons or muons (+neutrinos) about 17\% of the time each~\cite{PDG}. Those cases are almost indistinguishable from prompt production of the (lighter) leptons, as the only other produced particles are neutrinos. This Section focuses on fully hadronic decays.}.
Of the hadronic decays, over 90\% involve exactly $1$ or $3$ charged pions, up to two neutral pions, and a neutrino.

Tau-jets are identified~\cite{Aad:2014rga,ATL-PHYS-PUB-2015-045,Aad:2015unr,ATLAS-CONF-2017-029} given the number of tracks ($1$ or $3$), taking advantage of the narrow shower shape relative to quark- and gluon-initiated jets.
These low-level features are combined with a boosted decision tree into a final high-level tau-tagger.
Electrons (Section~\ref{sec:ATLAS:EM}) can pass the tau-jet identification with a single associated track; candidate tau-jets are removed if there is a nearby ($\Delta R<0.4$) electron.
Figure~\ref{fig:ATLAS:tautagging} shows the distribution of the high-level tagger for $1$- and $3$-prong jets initiated by quarks and gluons and for those initiated by taus.
Also shown is the background rejection rate as a function of the tau-jet efficiency; for a tau-jet efficiency around $80\%$, the background rejection is around $30$.

\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{tau_bdt_1prong}.png}}
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{tau_bdt_3prong}.png}}\\
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{tau_ROC}.png}}
  \caption{Distribution of scores from high-level tau-tagger for quark- and gluon-initiated jets (background) and for tau-jets (signal): (a) 1-prong jets; (b) 3-prong jets. (c) Background rejection rate vs efficiency on tau-jets. Some common working points are also included (not exactly on the lines due to implementing variable cuts intended to reduce the dependency of the efficiency on \pt). Figures sourced from~\cite{ATL-PHYS-PUB-2015-045}.}
  \label{fig:ATLAS:tautagging}
\end{figure}

The energy of tau-jets is further calibrated to the truth visible energy (i.e. not including the neutrinos), in a manner similar to that for generic jets (Section~\ref{sec:ATLAS:jet_calibration}).

The efficiency of the tau tagger and the energy calibration are measured in data with high-fidelity $Z\rightarrow \tau\tau$ events, where one of the taus decays to a muon to identify the event and the other decays hadronically.
Any differences between data and simulation are corrected, and uncertanties on the correction are applied as systematic uncertainties.

\subsection{Muons}
\label{sec:ATLAS:muons}
Muons are reconstructed~\cite{Aad:2014rra,Aad:2016jkr} as charged particle tracks in the muon system.
As almost all particles other than muons and neutrinos decay or shower in the calorimeter, muon identification has a very low background and events with muons in them can be used to identify specific physics topologies with high fidelity.
Figure~\ref{fig:ATLAS:VBFHmumu} shows an event display of an event likely corresponding to a Higgs boson (produced in the vector-boson-fusion mode) decaying to two muons.
\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{VBFHmumu}.png}}
  \caption{Event display showing candidate Higgs boson produced in the vector-boson-fusion mode decaying to two muons. Figure sourced from~\cite{EventDisplayRun2Physics}, related to~\cite{Aaboud:2017ojs}.}
  \label{fig:ATLAS:VBFHmumu}
\end{figure}

Muons are reconstructed from hits in the inner detector just like any other charged particle.
They are also reconstructed from hits in the various subsystems in the MS, using a Hough transform~\cite{ILLINGWORTH198887} to search for hits aligned along curved trajectories.
Track segments are formed in each layer of the MS and combined to form track candidates, fitting the segments and tracks with a $\chi^2$ fit to the hits.
Tracks in the MS and in the inner detector are combined and fit again to form full muon candidates.
Most muon candidates are formed outside-in (back-extrapolating MS tracks to inner detector tracks), but an inside-out method (extrapolating inner detector tracks to MS tracks) is used as a complementary approach.
In regions that the MS covers but the inner detector does not ($2.5<|\eta|<2.7$), muon candidates can be formed from information only in the MS.

Fake muons can come from a variety of sources: from charged hadrons, primarily pions and kaons, decaying to muons, which produces a kink in the track in the inner detector; from charged hadrons punching through to the MS, which lose significant energy in the calorimeter relative to the track momentum; and from cosmic muons, which do not originate from the primary vertex.
Each of these sources of background can be distinguished from prompt muons by testing the quality of the track in the inner detector and its compatibility with the track in the MS, especially the charge to momentum ratio.
A few different working points are defined by selecting on these features - the \texttt{Medium} working point, which is the default for analyses, has about a 95\% efficiency on prompt photons with $4<\pt<100$ GeV and $<0.5\%$ efficiency on charged pions decaying midflight to muons.

Muons can also be required to be isolated using similar variables as for photons and electrons (Section~\ref{sec:ATLAS:EM}), depending on the signal topology targeted in an analysis.

The efficiency on muons for the identification and isolation is measured in simulation and in data in high-fidelity $Z\rightarrow \mu\mu$ events, where one muon and the invariant mass $M_{\mu\mu}$ are used to tag the event, and the efficiency on the other muon is tested.
Differences in the efficiency between simulation and data are corrected, and uncertainties in this correction are applied as systematic uncertainties.

As muons are reconstructed as tracks in the MS, the charge-to-momentum ratio is measured very precisely.
The muon momentum is also compared between simulation and high-fidelity $Z\rightarrow \mu\mu$ events in data.
The differences between simulation and data in the momentum measurement are applied as corrections in the simulations and the uncertainties on this correction are applied as systematic uncertainties.

\subsection{Missing Energy}
\label{sec:ATLAS:met}
Because of momentum conservation, the vectorial transverse momentum of all objects in the event should sum to $0$\footnote{This statement is not necessarily true for the longitudinal momentum, as the colliding protons could have a momentum imbalance with each other.}.
Any missing transverse momentum in the event, or $\Etmiss$, could be due to invisible particles like neutrinos or new particles beyond the standard model.

$\Etmiss$ is calculated~\cite{Aad:2016nrq,Aaboud:2018tkc,ATLAS-CONF-2018-023} as the (opposite of the) vectorial sum of the transverse momentum from all other objects in the event - muons (Section~\ref{sec:ATLAS:muons}), electrons and photons (Section~\ref{sec:ATLAS:EM}), hadronically decaying tau jets (Section~\ref{sec:ATLAS:taus}), and all other jets (Section~\ref{sec:ATLAS:jets}) form the \textit{hard term}, and all energy deposits in the event not matched to jets (Section~\ref{sec:ATLAS:clusters}) form the \textit{soft term}.

For the hard term, objects are removed if they overlap with some better-measured object in order to avoid double counting; e.g., jets are removed if they overlap with a photon (which are reconstructed as jets), and only small-$R$ jets are used in the jet term (as large-$R$ jets are also reconstructed as small-$R$ jets).
Pile-up suppression in the central and forward region~\ref{sec:ATLAS:JVT} has a major effect on the resolution of \Etmiss{}, as pile-up jets are unrelated to the hard-scatter event and only add an additional source of noise due to mismeasurements of their energies or incomplete event reconstruction (even though in principle pile-up interactions should be \pt{} balanced themselves).
The jet term can use either calorimeter topoclusters as defined in Section~\ref{sec:ATLAS:clusters} or particle flow objects~\cite{Aaboud:2017aca}, which have been shown to improve the \Etmiss{} resolution.

The soft term is the hardest term to measure, as it accounts for energy deposits not reconstructed in other detector objects, which benefit from calibrations and pile-up suppression.
Because of this, the standard definition of the soft term is the track-based soft term (though different definitions have been used in the past~\cite{Aad:2012re,Aad:2016nrq}), which only uses all tracks with $\pt>400$ MeV associated to the hard scatter primary vertex and not associated to any other object in the event, via $\Delta R$ matching or ghost association for jets.
The gain in performance from excluding pile-up activity when using only tracks to measure the soft term makes up for the loss in performance due to not including neutral soft activity.

Despite this definition of the soft term, the $\Etmiss$ resolution gets worse as the number of primary vertices (NPV) increases.
Two working points are defined - a \texttt{Loose} working point using all jets with $\pt>20$ GeV, requiring central jets to pass JVT, and a \texttt{Tight} working point which in addition removes all forward jets with $20<\pt<30$ GeV.
There is in addition a middle working point that applies fJVT to forward jets.
The resolution of \Etmiss{} in $Z\rightarrow \mu\mu$ events (which in the dominant Drell-Yan production mode has no true missing energy) as a function of NPV can be seen in Figure~\ref{fig:ATLAS:etmiss}.
The resolution clearly gets worse with increasing pile-up, but the dependence gets less as the tightness of the forward pile-up cut gets tighter.

\begin{figure}[htbp]
  \centering 
  \subfloat[]{\includegraphics[width=0.5\textwidth]{figures/{etmiss_NPV}.pdf}}
  \caption{The dependence of the \Etmiss{} resolution on the number of primary vertices (NPV). The red circles correspond to the \texttt{Loose} working point; green boxes correspond to the \texttt{Tight} working point; and blue triangles correspond to the middle working point of \texttt{Loose}+fJVT. Figure sourced from~\cite{ATLAS-CONF-2018-023}.}
  \label{fig:ATLAS:etmiss}
\end{figure}

As all the underlying objects that go into \Etmiss{} are themselves calibrated and have associated systematic uncertainties, the \Etmiss{} itself is in principle calibrated (given the same definition in simulation and data), and the underlying object systematic uncertainties as propagated up as uncertainties on \Etmiss{}.
In addition, the \Etmiss{} distribution is compared between simulation and data in $Z\rightarrow ee/\mu\mu$ events, which can be identified with high fidelity, and any differences that exist are applied as systematic uncertainties.

There have been a variety of alternate definitions proposed for calculating \Etmiss.
One proposed method is to incorporate the \pt{} resolution of the individual objects to get an $\Etmiss$ significance~\cite{ATLAS-CONF-2018-038}.
An exciting alternative is to use a convolutional neural network to predict \Etmiss, treating the entire event as an image in $(\eta,\phi)$~\cite{ATL-PHYS-PUB-2019-028}.

