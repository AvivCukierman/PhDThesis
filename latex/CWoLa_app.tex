%\section{List of MC samples}
%\label{sec:CWoLa:app:CWoLa:samples}

%Signal
%
%\begin{lstlisting}[breaklines=true,basicstyle=\footnotesize\ttfamily]
%mc16_13TeV.450288.Pythia8EvtGen_A14NNPDF23LO_Wprime_WZqqqq_M6000_m200_m200.deriv.DAOD_EXOT3.e7206_s3126_r9364_p3665
%mc16_13TeV.450284.Pythia8EvtGen_A14NNPDF23LO_Wprime_WZqqqq_M3000_m400_m400.deriv.DAOD_EXOT3.e7206_s3126_r9364_p3665
%mc16_13TeV.450287.Pythia8EvtGen_A14NNPDF23LO_Wprime_WZqqqq_M6000_m80_m400.deriv.DAOD_EXOT3.e7206_s3126_r9364_p3665
%mc16_13TeV.450281.Pythia8EvtGen_A14NNPDF23LO_Wprime_WZqqqq_M3000_m80_m400.deriv.DAOD_EXOT3.e7206_s3126_r9364_p3665
%mc16_13TeV.450286.Pythia8EvtGen_A14NNPDF23LO_Wprime_WZqqqq_M6000_m80_m200.deriv.DAOD_EXOT3.e7206_s3126_r9364_p3665
%mc16_13TeV.450290.Pythia8EvtGen_A14NNPDF23LO_Wprime_WZqqqq_M6000_m400_m400.deriv.DAOD_EXOT3.e7206_s3126_r9364_p3665
%mc16_13TeV.450283.Pythia8EvtGen_A14NNPDF23LO_Wprime_WZqqqq_M3000_m200_m400.deriv.DAOD_EXOT3.e7206_s3126_r9364_p3665
%mc16_13TeV.450282.Pythia8EvtGen_A14NNPDF23LO_Wprime_WZqqqq_M3000_m200_m200.deriv.DAOD_EXOT3.e7206_s3126_r9364_p3665
%mc16_13TeV.450285.Pythia8EvtGen_A14NNPDF23LO_Wprime_WZqqqq_M6000_m80_m80.deriv.DAOD_EXOT3.e7206_s3126_r9364_p3665
%mc16_13TeV.450280.Pythia8EvtGen_A14NNPDF23LO_Wprime_WZqqqq_M3000_m80_m200.deriv.DAOD_EXOT3.e7206_s3126_r9364_p3665
%mc16_13TeV.450289.Pythia8EvtGen_A14NNPDF23LO_Wprime_WZqqqq_M6000_m200_m400.deriv.DAOD_EXOT3.e7206_s3126_r9364_p3665
%mc16_13TeV.450279.Pythia8EvtGen_A14NNPDF23LO_Wprime_WZqqqq_M3000_m80_m80.deriv.DAOD_EXOT3.e7206_s3126_r9364_p3665
%\end{lstlisting}

%\section{Neural Network Steps}
%\label{sec:CWoLa:app:CWoLa:NNsteps}

%\section{Background-only Neural Network Output}
%\label{sec:CWoLa:app:CWoLa:NNout_bkg}
%The regions that the neural network tags as ``signal-like", in the absence of any true signal, are shown in Figure~\ref{fig:CWoLa:NNout_sigRx}.
%\begin{figure}[htbp]
%  \centering 
%  \subfloat[]{\includegraphics[width=0.3\textwidth]{figures_CWoLa/{NNout_sigR1_12.31.18}.png}}
%  \subfloat[]{\includegraphics[width=0.3\textwidth]{figures_CWoLa/{NNout_sigR2_12.31.18}.png}}
%  \subfloat[]{\includegraphics[width=0.3\textwidth]{figures_CWoLa/{NNout_sigR3_12.31.18}.png}}\\
%  \subfloat[]{\includegraphics[width=0.3\textwidth]{figures_CWoLa/{NNout_sigR4_12.31.18}.png}}
%  \subfloat[]{\includegraphics[width=0.3\textwidth]{figures_CWoLa/{NNout_sigR5_12.31.18}.png}}
%  \subfloat[]{\includegraphics[width=0.3\textwidth]{figures_CWoLa/{NNout_sigR6_12.31.18}.png}}\\
%  \subfloat[]{\includegraphics[width=0.3\textwidth]{figures_CWoLa/{NNout_sigR7_12.31.18}.png}}
%  \subfloat[]{\includegraphics[width=0.3\textwidth]{figures_CWoLa/{NNout_sigR8_12.31.18}.png}}
%  \subfloat[]{\includegraphics[width=0.3\textwidth]{figures_CWoLa/{NNout_sigR9_12.31.18}.png}}\\
%  \caption{Neural network output in the absence of any true signal for signal regions (a) 1; (b) 2; (c) 3; (d) 4; (e) 5; (f) 6; (g) 7; (h) 8; (i) 9. Blue indicates more signal-like.}
%\label{fig:CWoLa:NNout_sigRx}
%\end{figure}

%\begin{figure}[htbp]
%  \centering 
%  \subfloat[]{\includegraphics[width=0.3\textwidth]{figures_CWoLa/{NNout_q99_sigR1_12.31.18_scatter}.png}}
%  \subfloat[]{\includegraphics[width=0.3\textwidth]{figures_CWoLa/{NNout_q99_sigR2_12.31.18_scatter}.png}}
%  \subfloat[]{\includegraphics[width=0.3\textwidth]{figures_CWoLa/{NNout_q99_sigR3_12.31.18_scatter}.png}}\\
%  \subfloat[]{\includegraphics[width=0.3\textwidth]{figures_CWoLa/{NNout_q99_sigR4_12.31.18_scatter}.png}}
%  \subfloat[]{\includegraphics[width=0.3\textwidth]{figures_CWoLa/{NNout_q99_sigR5_12.31.18_scatter}.png}}
%  \subfloat[]{\includegraphics[width=0.3\textwidth]{figures_CWoLa/{NNout_q99_sigR6_12.31.18_scatter}.png}}\\
%  \subfloat[]{\includegraphics[width=0.3\textwidth]{figures_CWoLa/{NNout_q99_sigR7_12.31.18_scatter}.png}}
%  \subfloat[]{\includegraphics[width=0.3\textwidth]{figures_CWoLa/{NNout_q99_sigR8_12.31.18_scatter}.png}}
%  \subfloat[]{\includegraphics[width=0.3\textwidth]{figures_CWoLa/{NNout_q99_sigR9_12.31.18_scatter}.png}}\\
%  \caption{Scatter plot of the events remaining after a $\epsilon=0.01$ neural network cut, in the absence of any true signal, in signal regions (a) 1; (b) 2; (c) 3; (d) 4; (e) 5; (f) 6; (g) 7; (h) 8; (i) 9.}
%\label{fig:CWoLa:NNout_q99_sigRx}
%\end{figure}

\section{Trials Factors and CWoLa}
\label{app:CWoLa:trials}
The search presented in Chapter~\ref{ch:CWoLa} uses Classification Without Labels (CWoLa) to be generically sensitive to a broad class of signal models, meaning that it does not specify a signal model ahead of time, but is able to tag signals based on features that discriminate them from the background and therefore be sensitive to whatever signal happens to be in the data\footnote{Subject to the constraints of the method and setup, as discussed in Section~\ref{sec:CWoLa:schematic}.}.
In the context of the search presented in Chapter~\ref{ch:CWoLa}, the features are the masses of the jets $m_1,m_2$\footnote{Here we are considering the search in a fixed $m_{JJ}$ bin.};
it should also be kept in mind that the natural extension of this search would use a suite of jet substructure variables so that the feature space is high-dimensional.

There are two other ways to be generically sensitive to a broad class of signal models.
The first approach is to not place any selections on the features at all - this approach could be called the ``Inclusive Search'' approach.
The analog of the Inclusive Search in the context of the search in Chapter~\ref{ch:CWoLa} would be the inclusive dijet search~\cite{Aad:2019hjw}.
A second way to be generically sensitive is to have a dedicated search in every region of the feature space - this could be referred to as the ``Direct Scanning'' approach.
The analog of the Direct Scanning approach would be a series of searches along the lines of the all-hadronic diboson resonance search~\cite{Aad:2019fbh}\footnote{In fact that search uses features other than just the jet mass to tag signals; including these other features and varying them with independent direct searches would be the analog of the natural extension of the search in Chapter~\ref{ch:CWoLa} mentioned above.}, which targets a region around $(m_1,m_2)=(80,80)~\GeV$ - in addition to that search, there could be another one targeting a region around $(m_1,m_2)=(200,200)~\GeV$, $(m_1,m_2)=(80,400)~\GeV$, etc.

It is crucially important to point out that CWoLa hunting differs fundamentally from these alternative approaches, in that it can be both more sensitive than the Inclusive Search, and in that it avoids suffering from a \textit{trials factor}, also known as a \textit{look-elsewhere effect}~\cite{miller_1966,mittelhammer_judge_miller_2000,Gross:2010qma,Blance:2019ibf}, that the Direct Scanning approach would be subject to.
This effect accounts for the fact that, with many statistically independent searches, there is a high probability of observing a large excess in at least one of the searches in the background-only case.
Therefore, the size of the excess needed to claim a discovery in any one search needs to be increased in order to lower the probability of a false discovery~\cite{bonferroni1936teoria,dunn1959,dunn1961}, ultimately reducing sensitivity to true signals.
The goal of this Section is to explain this effect in detail and how CWoLa hunting manages to avoid this effect, while still improving sensitivity compared to an inclusive search.

Suppose we collect a sample of $N$ data points $\mathbf{x} =\{x_1,...,x_N\}$, where each observation $x_i$ could be multi-dimensional corresponding to $n\geq 1$ features in each observation\footnote{E.g., $x_i = (m_1^i,m_2^i)$ for the $i$th event, in which case $n=2$.}.
$\mathbf{x}$ can be considered to be a sample from the random variable $\mathbf{X}$ which we observed by taking data.
We construct a \textit{test statistic}~\cite{Lyons:1900zz,Lyons:2018gtc,cowan_2004,barlow_2002,PDG} $Q(\mathbf{X})$ which is some mapping from the data to a real number, with the property that larger values of $Q$ correspond to increasing tension with the null or background-only hypothesis $H_0$.
The observed test statistic is $q=Q(\mathbf{x})$, and we define the observed $p$-value
\begin{align}
  p \equiv \text{Prob}(Q \geq q | H_0).
\end{align}
A small $p$-value therefore indicates incompatibility with the background-only hypothesis, and a possible discovery of a new signal.

The discovery procedure is to specify a threshold value $\alpha$ such that, if the observed $p\leq\alpha$, a discovery is claimed.
If the $p$-value is defined correctly, it should be the case that
\begin{align}
  \text{Prob}(p \leq \alpha | H_0) = \alpha,
\end{align}
i.e. the probability of making a Type I error (incorrectly rejecting the null hypothesis) is exactly $\alpha$.
If the probability is $<\alpha$ then the test is conservative, meaning that the probability of a Type I error is lower than expected; but if the probability is $>\alpha$ then the probability of a Type I error is higher than expected and the $p$-values should be recalibrated to account for this.

$\alpha$ is often mapped to a \textit{significance} $Z=\Phi^{-1}(1-\alpha)$, where $\Phi$ is the cumulative distribution function (CDF) of the normal distribution.

In particle physics, since the null hypothesis is the Standard Model, $\alpha$ is typically set very low - the ``$5-\sigma$ Standard''~\cite{Lyons:2013yja,Sinervo:2002sa} corresponds to $Z=5$, or $\alpha \sim 3\times10^{-7}$, so that the probability of falsely ruling out the SM is very low.

We set up a toy model to demonstrate the background-only $p$-values and signal discovery potential using the various generic signal searches outlined above.
In the toy model, the observed data are one-dimensional, $x_i \in [0,1]$\footnote{The toy methods outlined below will find small regions in this space of size $\epsilon$, corresponding to line segments $[y-\frac{\epsilon}{2},y+\frac{\epsilon}{2}]$. The multidimensional analog would be balls or hypercubes of volume $\epsilon$ in a space where the $n$ features have been mapped to $[0,1]^n$.}
Two samples will be considered: a background-only case ($B$ only) and a background plus signal case ($B+S$).
In the background-only case, there are expected $B$ background events\footnote{I.e., the number of background events is Poisson-distributed with mean $B$.} with values drawn from a uniform distribution $U(0,1)$.
In the background plus signal case, there are in addition expected $S$ signal events drawn from some very small but unknown region in this space $\mathcal{N}(y_S,\delta)$, with $\delta\ll 1$\footnote{In $n$ dimensions, the volume taken up by the signal in $[0,1]^n$ space would decrease quickly with $n$, so this assumption is realistic. Another way to say this is that, as the number of features increases, a selection with fixed signal efficiency can have a background efficiency $\delta \ll 1$.}.
In these examples $S$ has been set to $\sqrt{B}$, i.e. a $1-\sigma$ excess over the background-only hypothesis overall.

The first approach is the Inclusive Search, in which the test statistic $Q$ is the total number of events observed.
In the background-only hypothesis $H_0$, $Q\sim\text{Poiss}(B)$, with the $p$-values defined according to the Poisson CDF.
Note that, in this case and in all cases outlined below, $B$ is estimated from some external information; e.g. in the search in Chapter~\ref{ch:CWoLa} $B$ is estimated via the $m_{JJ}$ fit interpolated in the $m_{JJ}$ region being studied.
Figure~\ref{fig:CWoLa:p_all} shows the probability distributions of the $p$-values.
The background-only case has $\text{Prob}(p \leq \alpha | H_0) = \alpha$ as expected, and the background plus signal case has probability $>\alpha$, indicating tension with the background-only hypothesis.
In particular, the probability of observing a $Z=1$ significance is $50\%$, which makes sense since the injected signal corresponds to a $1-\sigma$ excess.
As a point of reference, the probability of observing a $Z=2$ significance is $\sim 0.16$ and the probability of observing a $Z=3$ significance is $\sim 0.02$ with this amount of signal.

\begin{figure}[h!]
\centering
%\subfloat[]{\includegraphics[width=0.45\textwidth]{/Users/acukierm/Documents/ATLAS/CWoLa/ToyMC/{p_all}.png}}
\includegraphics[width=0.8\textwidth]{figures_CWoLa/{trials_p_all}.png}
\caption{Probability distributions of $p$-values for the Inclusive Search in the (blue) background-only and (red) background plus signal cases. The black dotted line indicates the ideal background-only case of $\text{Prob}(p \leq \alpha | H_0) = \alpha$. Then green dashed lines indicate the $\alpha$ thresholds corresponding to integer significances $Z$.}
\label{fig:CWoLa:p_all}
\end{figure}

The second approach is the Direct Scanning method.
In this approach, the $[0,1]$ space is partitioned into regions of size $\epsilon=0.1$, $[0,\epsilon,2\epsilon,...,1]$.
In each region a signal discovery test is performed, corresponding to discrete searches covering all signal model hypotheses, with $Q$ the number of events in the region and $Q\sim\text{Poiss}(\epsilon B)$ the background-only hypothesis.
The overall $p$-value (also known as the \textit{global} $p$-value) is taken as the minimum $p$-value over all the regions (i.e., $p<\alpha$ means that at least one of these searches claimed a discovery).
Figure~\ref{fig:CWoLa:p_max} shows the probability distributions of the $p$-values.
The background-only case has $\text{Prob}(p \leq \alpha | H_0) > \alpha$, since there is a high probability that at least one of the tests has a low $p$-value.
This effect is exactly what is referred to as a trials factor or look-elsewhere effect.
In principle this global $p$-value can be corrected analytically if the number of tests is fixed~\cite{bonferroni1936teoria,dunn1959,dunn1961} or calibrated by simulating the entire procedure in some sample intended to produce the background-only $p$-value distribution.
In this example, the analytic correction would require $\alpha \rightarrow \hat{\alpha} \approx \frac{\alpha}{m}$\footnote{The more precise correction is actually $\alpha = 1-(1-\hat{\alpha})^m$.}, with $m=10$ the number of tests, for the same significance level;
in the simulation it can also be seen that $\text{Prob}(p \leq \hat{\alpha} | H_0) \approx \alpha$.
However requiring this more stringent test reduces the signal sensitivity - for example, the probability of observing a $Z=2$ significance ($\alpha\approx 0.02 \rightarrow \hat{\alpha}\approx 0.002$) is approximately $0.16$, the same as in the Inclusive Search\footnote{Actually it seems that even with this correction signal sensitivity is slightly higher than in the Inclusive Search, since the true signal does lie in one of the regions and there is some chance that all the other regions have high $p$-values. However the improvement is minimal compared to the improvement that can be made with CWoLa, as will be seen below.}.

In any case, there are good reasons to avoid having to re-calibrate the background-only $p$-values: most often the disparate searches will not be so clear-cut as to allow an analytic correction; and a simulation of the background-only $p$-values is often hard to come by.
Because of this, it's desirable to have a method where the background-only $p$-values are defined correctly.

\begin{figure}[h!]
\centering
%\subfloat[]{\includegraphics[width=0.45\textwidth]{/Users/acukierm/Documents/ATLAS/CWoLa/ToyMC/{p_all}.png}}
\includegraphics[width=0.8\textwidth]{figures_CWoLa/{trials_p_max}.png}
\caption{Probability distributions of $p$-values for the Direct Scanning approach in the (blue) background-only and (red) background plus signal cases. The black dotted line indicates the ideal background-only case of $\text{Prob}(p \leq \alpha | H_0) = \alpha$. Then green dashed lines indicate the $\alpha$ thresholds corresponding to integer significances $Z$.}
\label{fig:CWoLa:p_max}
\end{figure}

We turn now to the ideas that enable CWoLa to be more sensitive than the Inclusive Search without biasing the background-only $p$-values.
The first thing to point out is that the trials factor can be removed by splitting the data randomly into a train set comprising $(1-f)$ of the data and a test set comprising $f$ of the data.
In the \textit{train} set the Direct Scanning method is performed to find the region with minimum $p$-value, but the test statistic is then defined to be the number of events in the \textit{test} set in the region indicated by the train set, which under the background-only hypothesis should be distributed as $\text{Poiss}(f\epsilon B)$.
Since the train and tests sets are statistically independent in the background-only case, just because there's an excess in the train set that does not imply that there will be an excess in the test set, and the background-only $p$-values should be unbiased.

Figure~\ref{fig:CWoLa:p_traintest} shows the probability distributions of the $p$-values with a $f=\frac{1}{2}$ train/test split.
The background-only case has $\text{Prob}(p \leq \alpha | H_0) = \alpha$ as expected.
However, the signal sensitivity is actually worse than the Inclusive Search.
This is due to two factors.
First, since $(1-f)<1$, it is harder to find the region in which the signal lays than without the train/test split.
Second, since $f<1$, even in cases where the correct region is found, the significance of the excess in that region is reduced by a factor of $\sim\sqrt{f}$\footnote{The significance is roughly $\frac{S}{\sqrt{B}}$. If both $S$ and $B$ are multiplied by a factor of $f<1$, the significance is multiplied by $\sqrt{f}<1$.}.

\begin{figure}[h!]
\centering
%\subfloat[]{\includegraphics[width=0.45\textwidth]{/Users/acukierm/Documents/ATLAS/CWoLa/ToyMC/{p_all}.png}}
\includegraphics[width=0.8\textwidth]{figures_CWoLa/{trials_p_traintest}.png}
\caption{Probability distributions of $p$-values for the Direct Scanning approach with a $f=\frac{1}{2}$ train/test split in the (blue) background-only and (red) background plus signal cases. The black dotted line indicates the ideal background-only case of $\text{Prob}(p \leq \alpha | H_0) = \alpha$. Then green dashed lines indicate the $\alpha$ thresholds corresponding to integer significances $Z$.}
\label{fig:CWoLa:p_traintest}
\end{figure}

Both of these factors can be addressed by having a $k$-fold cross-testing.
In this method, the data are split randomly into $k$ sets.
One of the $k$ sets is set as the test set, comprising $f=\frac{1}{k}$ of the data, and the remaining $1-f=\frac{k-1}{k}$ of the data is set as the train set.
The Direct Scanning method with the train/test split described above is then performed.
Then, the whole process is repeated with each of the $k$ sets designated as the test set in turn.
The test statistic is the sum across the $k$ test sets of the number of events in the chosen region from the respective train set, which in the background-only hypothesis is distributed as $\text{Poiss}(\epsilon B)$.
Both of the factors mentioned above are addressed, as the train set comprises $\frac{k-1}{k} \sim 1$ of the data, and the sum of the test set sizes is not reduced by a factor of $f$.

Figure~\ref{fig:CWoLa:p_traintest_kfold} shows the probability distributions of the $p$-values with a $k=5$ $k$-fold cross-testing.
Unfortunately, the background-only $p$-values are biased, with $\text{Prob}(p \leq \alpha | H_0) > \alpha$.
In cases where there is a large (fake) excess in one of the regions, even when splitting into the $k$ sets, there will still be a large excess in each of the $k$ train sets in that region.
This means that each of the $k$ trainings will choose the same region to tag in the test set, which is the region in which there is a large excess overall.
Interestingly, this effect goes away when there are no large excesses in any of the regions, which can be seen in Figure~\ref{fig:CWoLa:p_traintest_kfold} as $\text{Prob}(p \leq \alpha | H_0) = \alpha$ for $\alpha\gtrsim0.3$.
This effect is made more clear in the case that $k\gg1$ - in that case, the training is basically the same every time, so that the same region is always chosen for the test set.

\begin{figure}[h!]
\centering
%\subfloat[]{\includegraphics[width=0.45\textwidth]{/Users/acukierm/Documents/ATLAS/CWoLa/ToyMC/{p_all}.png}}
\includegraphics[width=0.8\textwidth]{figures_CWoLa/{trials_p_traintest_kfold}.png}
\caption{Probability distributions of $p$-values with a $k=5$ $k$-fold cross-testing in the (blue) background-only and (red) background plus signal cases. The black dotted line indicates the ideal background-only case of $\text{Prob}(p \leq \alpha | H_0) = \alpha$. Then green dashed lines indicate the $\alpha$ thresholds corresponding to integer significances $Z$.}
\label{fig:CWoLa:p_traintest_kfold}
\end{figure}

CWoLa hunting makes use of the above ideas with $k$-fold cross-testing.
However, in addition, CWoLa uses a neural network (NN) to learn what region to tag in the train set before tagging in the test set.
The NN features a key difference to the Direct Scanning method outlined above in that the NN score has certain regularization requirements which lead to smoothness in the NN output\footnote{In the search in Chapter~\ref{ch:CWoLa}, there is also a validation set which serves to prevent overtraining the network, and the scores from multiple networks across different validation permutations are averaged together, further smoothing the output.}.
The NN effectively assumes the probability distribution of $x$ is smooth, by finding small regions $\epsilon'<\epsilon$ with overdensities, assuming that corresponds to a local maximum of the probability distribution, and smoothly interpolating from there.
In the toy model, this behavior is encapsulated by finding the region with length $\epsilon' = \frac{\epsilon}{10} = 0.01$ that has the highest density in the train set, and then designating the region of size $\epsilon$ with the overdensity at its center as the tagging region for the test set.

To demonstrate the advantages of the NN over the Direct Scanning method, this mock NN is demonstrated \textbf{without $k$-fold cross-testing}, but rather training and testing on the same set.
The test statistic is the number of events observed in the tagging region, which in the background-only hypothesis should be distributed as $\text{Poiss}(\epsilon B)$.

The probability distributions of the $p$-values with this mock NN are shown in Figure~\ref{fig:CWoLa:p_nokfold}.
Despite training and testing on the same set, as in the original Direct Scanning approach (Figure~\ref{fig:CWoLa:p_max}), the background-only $p$-values already look better, albeit still biased (since there is an overdensity in the tagged region).
In addition, the $p$-values in the background plus signal case are significantly higher than in the Direct Scanning approach, indicating that if the background-only $p$-values were calibrated, the signal sensitivity would be better.

\begin{figure}[h!]
\centering
%\subfloat[]{\includegraphics[width=0.45\textwidth]{/Users/acukierm/Documents/ATLAS/CWoLa/ToyMC/{p_all}.png}}
\includegraphics[width=0.8\textwidth]{figures_CWoLa/{trials_p_nokfold}.png}
\caption{Probability distributions of $p$-values with a mock NN with no $k$-fold cross-testing in the (blue) background-only and (red) background plus signal cases. The black dotted line indicates the ideal background-only case of $\text{Prob}(p \leq \alpha | H_0) = \alpha$. Then green dashed lines indicate the $\alpha$ thresholds corresponding to integer significances $Z$.}
\label{fig:CWoLa:p_nokfold}
\end{figure}

The CWoLa hunting method makes use of both a NN and a $k$-fold cross-testing ($k=5$), combining the advantages of both innovations over the Direct Scanning method.
The test statistic is the number of events observed in the tagging region summed across the test sets, which in the background-only hypothesis should be distributed as $\text{Poiss}(\epsilon B)$.

The probability distributions of the $p$-values can be seen in Figure~\ref{fig:CWoLa:p_kfold}.
It can be seen that the background-only $p$-values are approximately unbiased, with $\text{Prob}(p \leq \alpha | H_0) \approx \alpha$ across the whole range.
In addition, the background plus signal $p$-values are significantly higher than in the Inclusive Search - the probability of a $Z=2$ excess is about $0.9$, and the probability of a $Z=3$ excess is greater than $0.5$.
This indicates that the signal sensitivity is significantly better than in the Inclusive Search, without paying a penalty of higher Type I errors.

\begin{figure}[h!]
\centering
%\subfloat[]{\includegraphics[width=0.45\textwidth]{/Users/acukierm/Documents/ATLAS/CWoLa/ToyMC/{p_all}.png}}
\includegraphics[width=0.8\textwidth]{figures_CWoLa/{trials_p_kfold}.png}
\caption{Probability distributions of $p$-values with a mock NN and with $k=5$ $k$-fold cross-testing in the (blue) background-only and (red) background plus signal cases. The black dotted line indicates the ideal background-only case of $\text{Prob}(p \leq \alpha | H_0) = \alpha$. Then green dashed lines indicate the $\alpha$ thresholds corresponding to integer significances $Z$.}
\label{fig:CWoLa:p_kfold}
\end{figure}

It should be emphasized that the location of the signal $y_S$ was never specified, and so the NN is able to find the signal regardless of where it lies in the feature space.
Thus, the CWoLa hunting method is generically sensitive without paying a large trials factor, as advertised.

In this toy model the size of the signal $\delta$ was set to be smaller than $\epsilon'$ in order to demonstrate how CWoLa hunting can find signals without biasing the background-only $p$-values.
However, this is not necessary in general, as long as the presence of the signal causes a significant overdensity in a small region, which the NN can tag according to its regularization requirements. 

In addition, the above discussion points out that the $k$-fold cross-testing is necessary for the CWoLa hunting method to work as expected, but the use of a regularized NN is equally important, which is maybe not as obvious.
The various complicated steps of the NN training (Section~\ref{sec:CWoLa:network}) serve not only to prevent overtraining the network but also to smooth the NN output and meet these smoothness requirements.

Finally, the above discussion is simply a toy model intended to demonstrate how CWoLa hunting can work, but not necessarily that it does, which depends on the NN architecture.
Ultimately the background-only $p$-values have to be validated as an empirical test of the setup.
These tests are included in the various steps of the validation (Sections~\ref{sec:CWoLa:simulation_analysis} and~\ref{sec:CWoLa:val_analysis}).

\section{Alternate Ideas for Validation}
\label{app:CWoLa:validation_alternate}
Section~\ref{sec:CWoLa:blinding} discusses the difficulties present in this analysis for constructing a validation sample with low signal efficiency to test the analysis pipeline.
The primary validation sample used in this analysis is formed by inverting the delta rapidity cut.
Some other ideas for forming a validation sample are below, which could be useful for a future analysis based on classification without labels, e.g. one that does not want to impose a delta rapidity cut or one targeting a different topology entirely.

\subsection{Swapped Dataset}
One possibility for a validation sample which is derived from data, and still retains the property that the signal contamination is low, is called the \textit{jet swapped} dataset.
In this dataset, leading and subleading jets are swapped between random pairs of events.
It is expected that this sample has low signal contamination, because it is unlikely that two random events chosen from the original sample will both be signal events.
In particular, suppose that there is some number of signal, $S$, and some number of background, $B$, in the original sample with no cuts.
We can suppose that the signal fraction $\frac{S}{B} = p\ll1$, due to existing limits from the inclusive dijet search~\cite{Aad:2019hjw} (note that the limits from previous searches set a stronger bound, that the significance $\frac{S}{\sqrt{B}}=p\sqrt{B} \lesssim 1$).
Then the signal contamination in the swapped dataset is expected to be on the order of $p^2 \ll p$ (and therefore with significance $p^2\sqrt{B} \ll 1$).

The $m_{JJ}$ distribution in the swapped dataset is different than in the original sample, and some correlations between the ensemble of jet features and the $m_{JJ}$ value will not be preserved.
However, any correlations between the individual jet features and the jet $p_T$ will be preserved, and thus some significant part of the correlations between the individual jet features and the $m_{JJ}$ value.
The swapped dataset therefore serves as an entirely data-derived reasonable proxy for the background spectrum of the features and of $m_{JJ}$, and can be used as a validation region for testing the validity of the background shape fit, for demonstrating that the learned features do not sculpt the $m_{JJ}$ distribution and therefore violate Assumption~\ref{ass2}, and for demonstrating the sensitivity of the method to (unswapped) injected signal.
%In particular, if the learned cuts sculpted the $m_{JJ}$ spectrum based on the individual jet features, the jet swapped dataset would also observe this effect.

\subsection{Anti-tagged Dataset}
After learning the neural network scores to distinguish events in the signal region from events in the sideband regions, cuts are placed at some efficiency $\epsilon < 1$ in order to find the most signal-like events.
That is to say, every event $i$ has a score $0\le S_i \le 1$ based on the neural network output, with larger $S_i$ indicating an event more signal-like, and moreover the scores are scaled such that a cut $S_i>1-\epsilon$ has efficiency exactly $\epsilon$ in the signal region bin.

There is a concept of anti-tagging in this framework: by applying a cut $S_i<\epsilon$, the $\epsilon$ fraction of events that are the least signal-like are chosen.
However, this anti-tagged dataset may still be contaminated by signal, in particular if the true signal is actually mostly in the sideband region rather than the signal region i.e. Case~\ref{case2}; in this case the scores $S_i$ are signal anti-taggers in the first place, and so the anti-tagged dataset is in fact anti-anti-tagging signal, i.e. positively tagging signal.

\subsection{Median Dataset}
Another possibility is to consider the \textit{median dataset}, where the cut that is placed is $|S_i-0.5|<\frac{\epsilon}{2}$.
These are the set of events that the neural network has decided it is agnostic about being in the signal region or the sideband regions; it is therefore expected that this dataset has little signal contamination, because regardless of whether the true signal was in the signal or sideband regions, it would not end up with this median score.
It is not expected that the $m_{JJ}$ distribution will be exactly the same in the median dataset as in the signal-tagged dataset, even in the case there is no true signal, since there are some residual correlations between the features and $m_{JJ}$.
However, the median dataset can be used to test the validity of the background shape fit, and for looking at jet kinematics in a blinded way.

\section{Analysis Software}
\label{app:CWoLa:software}

The \href{https://gitlab.cern.ch/acukierm/mc16-xAOD-ntuple-maker}{mc16-xAOD-ntuple-maker} package is used to create ntuples used for the analysis.
The machine learning code can be found in \href{https://gitlab.cern.ch/cwola-hunting/cwola-hunting-learning}{cwola-hunting-learning} package.
%The final fit was performed using the \href{https://gitlab.cern.ch/atlas-phys/exot/dbl/ResonanceFinder}{ResonanceFinder} package.   Our modifications to this package can be found \href{https://gitlab.cern.ch/cwola-hunting/fitting/tree/minimal}{here}.

\section{Fitting Software}
\label{app:CWoLa:fitting_software}
The statistical analysis uses the \href{https://gitlab.cern.ch/atlas-phys/exot/dbl/ResonanceFinder}{\texttt{ResonanceFinder}} package from the DBL group.
This code is based on RooFit~\cite{Verkerke:2003ir}, RooStats~\cite{Moneta:2010pm}, and HistFactory~\cite{Cranmer:1456844}.
Significant modifications for the background fitting were made for this analysis, which can be found \href{https://gitlab.cern.ch/cwola-hunting/fitting/tree/minimal}{here}.


\section{Bin Offset Test}
\label{app:CWoLa:binoffset}
A study of the effect of the bin positioning relative to the signal center $m_A$ is shown in Figure~\ref{fig:CWoLa:NN_binoffset}.
A fixed signal is injected with $m_A=3000$ GeV, so that the $m_{JJ}$ distribution of the signal lies mostly in signal region 5.
For this study only, the signal region bins are shifted in units of $0.25$ the current bin size, so that after $4$ shifts the bins are exactly the same as before with the numbering changed by $1$.
The NN efficiency on the signal at $\epsilon=0.1$ is shown as a function of the center of signal region 5, when the signal region used for training is 4, 5, and 6.
It can be seen that, regardless of where the bin definitions are, there is some signal region for which the NN efficiency is high ($>0.4$).
Importantly, the NN maintains very high signal efficiency ($>0.8$) in 3 out of the 4 bin positions, and dips lower in only the final 1 out of 4 positions.
This indicates that if the signal $m_A$ lies in roughly $75\%$ of the $m_{JJ}$ kinematic space, the NN performance is unaffected, while in the remaining $25\%$ of the space the NN can still learn to tag the signal albeit at lower efficiency; regardless, CWoLa is sensitive to these new signals.

\begin{figure}[h!]
\centering
\includegraphics[width=0.45\textwidth]{figures_CWoLa/{NN_binoffset_q90_sigR456_Wprime_WZqqqq_M3000_m200_m200_patience100_1.14.20}.pdf}
\caption{The efficiency of the NN on the signal ($m_A,m_B,m_C=(3000,200,200)$ GeV) at cut $\epsilon=0.1$ at different values of the center of signal region 5.
The efficiency of the NN is shown when training on signal region 4 (in red); 5 (in blue); and 6 (in green).
The distribution of $m_{JJ}$ in the signal is also shown (in orange) for reference.}
\label{fig:CWoLa:NN_binoffset}
\end{figure}


%\section{Validation Data: No Signal NN Output}
%\label{app:CWoLa:inverted:NNout}
%\begin{figure}[h!]
%\centering
%\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{NNoutcontrast_sigR6_patience100_yinvertresample_11.2.19}.pdf}}
%subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{NNoutcontrast_sigR7_patience100_yinvertresample_11.2.19}.pdf}}\\
%\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{NNoutcontrast_sigR8_patience100_yinvertresample_11.2.19}.pdf}}
%\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{NNoutcontrast_sigR9_patience100_yinvertresample_11.2.19}.pdf}}\\
%\caption{The output of the neural network when there is no injected signal, in signal region (a) 6; (b) 7; (c) 8; (d) 9. Note that these are data and not simulation, using the inverted rapidity cut data selection.}
%\label{fig:CWoLa:inverted:NNsensitivity:nosignal_app}
%\end{figure}

\FloatBarrier
\section{Validation Analysis: No Signal Fits}
\label{app:CWoLa:inverted:fit_nosignal}
The fit on the validation data with no signal injected is shown for signal region 6 (Figure~\ref{fig:CWoLa:inverted:fit_sigR6_nosignal}); signal region 7 (Figure~\ref{fig:CWoLa:inverted:fit_sigR7_nosignal}); signal region 8 (Figure~\ref{fig:CWoLa:inverted:fit_sigR8_nosignal}); and signal region 9 (Figure~\ref{fig:CWoLa:inverted:fit_sigR9_nosignal}).
These fits generally indicate problems with the $\epsilon=1.0$ and $\epsilon=0.25$ fits, especially at lower masses, which motivates the decision to limit the analysis to only $\epsilon=0.1$ and $\epsilon=0.01$ for the full unblinded analysis.
\begin{figure}[h!]
\centering
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{sigR6_patience100_yinvertresample_11.2.19_SWiFT_all}.pdf}}
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{q75_sigR6_patience100_yinvertresample_11.2.19_SWiFT_all}.pdf}}\\
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{q90_sigR6_patience100_yinvertresample_11.2.19_SWiFT_all}.pdf}}
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{q99_sigR6_patience100_yinvertresample_11.2.19_SWiFT_all}.pdf}}
\caption{The background fit when there is no injected signal, in signal region 6, for various efficiency points $\epsilon$. Note that these are data and not simulation, using the inverted rapidity cut data selection.}
\label{fig:CWoLa:inverted:fit_sigR6_nosignal}
\end{figure}

\begin{figure}[h!]
\centering
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{sigR7_patience100_yinvertresample_11.2.19_SWiFT_all}.pdf}}
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{q75_sigR7_patience100_yinvertresample_11.2.19_SWiFT_all}.pdf}}\\
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{q90_sigR7_patience100_yinvertresample_11.2.19_SWiFT_all}.pdf}}
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{q99_sigR7_patience100_yinvertresample_11.2.19_SWiFT_all}.pdf}}
\caption{The background fit when there is no injected signal, in signal region 7, for various efficiency points $\epsilon$. Note that these are data and not simulation, using the inverted rapidity cut data selection.}
\label{fig:CWoLa:inverted:fit_sigR7_nosignal}
\end{figure}

\begin{figure}[h!]
\centering
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{sigR8_patience100_yinvertresample_11.2.19_SWiFT_all}.pdf}}
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{q75_sigR8_patience100_yinvertresample_11.2.19_SWiFT_all}.pdf}}\\
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{q90_sigR8_patience100_yinvertresample_11.2.19_SWiFT_all}.pdf}}
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{q99_sigR8_patience100_yinvertresample_11.2.19_SWiFT_all}.pdf}}
\caption{The background fit when there is no injected signal, in signal region 8, for various efficiency points $\epsilon$. Note that these are data and not simulation, using the inverted rapidity cut data selection.}
\label{fig:CWoLa:inverted:fit_sigR8_nosignal}
\end{figure}

\begin{figure}[h!]
\centering
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{sigR9_patience100_yinvertresample_11.2.19_SWiFT_all}.pdf}}
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{q75_sigR9_patience100_yinvertresample_11.2.19_SWiFT_all}.pdf}}\\
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{q90_sigR9_patience100_yinvertresample_11.2.19_SWiFT_all}.pdf}}
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{q99_sigR9_patience100_yinvertresample_11.2.19_SWiFT_all}.pdf}}
\caption{The background fit when there is no injected signal, in signal region 9, for various efficiency points $\epsilon$. Note that these are data and not simulation, using the inverted rapidity cut data selection.}
\label{fig:CWoLa:inverted:fit_sigR9_nosignal}
\end{figure}

%\section{Validation Data: Injected Signal Fits}
%\label{app:CWoLa:inverted:fit_signal}

\FloatBarrier
\section{Unblinded Analysis: Signal Injection Tests}
\label{app:CWoLa:signalinjection}

The effect of an injected signal on the fit is studied.
Figure~\ref{fig:CWoLa:signalinjection} shows the dependence of the maximum likelihood signal strength $\hat{\mu}$ on the injected signal strength $\mu=\mathcal{L}\times\sigma$, where $\mathcal{L}$ is the data luminosity and $\sigma$ is the cross section for the production of the given signal.
Also indicated is the final 95\% CL exclusion limit on the given signal.
It can be seen that the fitted signal strength is not consistent with the injected signal strength for values below the 95\% CL exclusion limit; above that value there is some bias towards smaller values due to the fit process outlined in Section~\ref{sec:CWoLa:unblinded:fitting}.

\begin{figure}[h!]
\centering
\subfloat[]{\includegraphics[width=0.33\textwidth]{figures_CWoLa/{inj_closure_M3000_m200_m200_q90_3000}.pdf}}
\subfloat[]{\includegraphics[width=0.33\textwidth]{figures_CWoLa/{inj_closure_M3000_m200_m400_q90_3000}.pdf}}
\subfloat[]{\includegraphics[width=0.33\textwidth]{figures_CWoLa/{inj_closure_M3000_m400_m400_q90_3000}.pdf}}\\
\subfloat[]{\includegraphics[width=0.33\textwidth]{figures_CWoLa/{inj_closure_M5000_m200_m200_q90_5000}.pdf}}
\subfloat[]{\includegraphics[width=0.33\textwidth]{figures_CWoLa/{inj_closure_M5000_m200_m400_q90_5000}.pdf}}
\subfloat[]{\includegraphics[width=0.33\textwidth]{figures_CWoLa/{inj_closure_M5000_m400_m400_q90_5000}.pdf}}
\caption{The dependence of the fitted signal strength $\hat{\mu}$ on the injected signal strength $\mu$ with a NN cut at efficiency $\epsilon=0.1$ trained on (a,b,c) signal region 5 and (d,e,f) signal region 8 for a signal with $m_A,m_B,m_C$ equal to (a) (3000,200,200); (b) (3000,400,200); (c) (3000,400,400); (d) (5000,200,200); (e) (5000,400,200); and (f) (5000,400,400) GeV. Also shown is the 95\% CL exclusion limit for the given signal.}
\label{fig:CWoLa:signalinjection}
\end{figure}

\FloatBarrier
\section{Unblinded Analysis: Neural Network Dependence on $\mu$}
\label{app:CWoLa:unblinded:NNeff_mu}
The NN efficiency on the given signals as a function of $\mu$ is given in Figure~\ref{fig:CWoLa:unblinded:NNeff_mu_3000} for signals with $m_A=3000$ GeV and in Figure~\ref{fig:CWoLa:unblinded:NNeff_mu_5000} for signals with $m_A=5000$ GeV.
The NN output for each of the $5$ different random samplings of the signal is included.
The NN with the median efficiency on the signal is also indicated; note that this does not necessarily correspond to the NN that gives rise to the median expected limit, because the shape of the background may change, and so this is only indicated as an aesthetic choice.
The envelope of the outputs across the different random samplings tends to be small when the efficiency is very high or very low, while the envelope widens at the $\mu$ values where the efficiency is middling; these are exactly the transition regions where the NN can find the signal but does not always.
The median NN tends to be smoothly rising with $\mu$, while single samplings from the envelope may not be; thus, the choice to run the analysis with different random samplings serves as a smoothing procedure.
For $m_A=5000$ GeV, for the best-performing signals at high $m_B,m_C$, the NN efficiency actually goes down with increasing $\mu$.
This is simply due to the fact that at these values of $\mu$ the amount of signal is comparable to the $10\%$ or $1\%$ of the signal plus background remaining after the NN tagging, so that it is mathematically impossible to have a higher efficiency on the signal and retain the overall efficiency on all the events in the signal region.

\begin{figure}[h!]
\centering
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{NNeff_mu_q90_sigR5_Wprime_WZqqqq_M3000_m80_m80_patience100_1.14.20}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{NNeff_mu_q99_sigR5_Wprime_WZqqqq_M3000_m80_m80_patience100_1.14.20}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{NNeff_mu_q90_sigR5_Wprime_WZqqqq_M3000_m80_m200_patience100_1.14.20}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{NNeff_mu_q99_sigR5_Wprime_WZqqqq_M3000_m80_m200_patience100_1.14.20}.pdf}}\\
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{NNeff_mu_q90_sigR5_Wprime_WZqqqq_M3000_m80_m400_patience100_1.14.20}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{NNeff_mu_q99_sigR5_Wprime_WZqqqq_M3000_m80_m400_patience100_1.14.20}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{NNeff_mu_q90_sigR5_Wprime_WZqqqq_M3000_m200_m200_patience100_1.14.20}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{NNeff_mu_q99_sigR5_Wprime_WZqqqq_M3000_m200_m200_patience100_1.14.20}.pdf}}\\
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{NNeff_mu_q90_sigR5_Wprime_WZqqqq_M3000_m200_m400_patience100_1.14.20}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{NNeff_mu_q99_sigR5_Wprime_WZqqqq_M3000_m200_m400_patience100_1.14.20}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{NNeff_mu_q90_sigR5_Wprime_WZqqqq_M3000_m400_m400_patience100_1.14.20}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{NNeff_mu_q99_sigR5_Wprime_WZqqqq_M3000_m400_m400_patience100_1.14.20}.pdf}}\\
\caption{The efficiency of the NN as a function of $\mu$ on the injected signal at $m_A=3000$, in signal region 5, for (a,c,e,g,i,k) $\epsilon=0.1$ and (b,d,f,h,j,l) $\epsilon=0.01$. 
  There are 5 lines corresponding to the $5$ different random samplings of the signal in the training of the NN; the network with the median efficiciency is also marked.
  Each signal is labeled by ($m_B,m_C$) in \GeV. (a,b) (80,80); (c,d) (80,200); (e,f) (80,400); (g,h) (200,200); (i,j) (200,400); (k,l) (400,400).
}
\label{fig:CWoLa:unblinded:NNeff_mu_3000}
\end{figure}

\begin{figure}[h!]
\centering
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{NNeff_mu_q90_sigR8_Wprime_WZqqqq_M5000_m80_m80_patience100_1.14.20}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{NNeff_mu_q99_sigR8_Wprime_WZqqqq_M5000_m80_m80_patience100_1.14.20}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{NNeff_mu_q90_sigR8_Wprime_WZqqqq_M5000_m80_m200_patience100_1.14.20}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{NNeff_mu_q99_sigR8_Wprime_WZqqqq_M5000_m80_m200_patience100_1.14.20}.pdf}}\\
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{NNeff_mu_q90_sigR8_Wprime_WZqqqq_M5000_m80_m400_patience100_1.14.20}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{NNeff_mu_q99_sigR8_Wprime_WZqqqq_M5000_m80_m400_patience100_1.14.20}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{NNeff_mu_q90_sigR8_Wprime_WZqqqq_M5000_m200_m200_patience100_1.14.20}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{NNeff_mu_q99_sigR8_Wprime_WZqqqq_M5000_m200_m200_patience100_1.14.20}.pdf}}\\
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{NNeff_mu_q90_sigR8_Wprime_WZqqqq_M5000_m200_m400_patience100_1.14.20}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{NNeff_mu_q99_sigR8_Wprime_WZqqqq_M5000_m200_m400_patience100_1.14.20}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{NNeff_mu_q90_sigR8_Wprime_WZqqqq_M5000_m400_m400_patience100_1.14.20}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{NNeff_mu_q99_sigR8_Wprime_WZqqqq_M5000_m400_m400_patience100_1.14.20}.pdf}}\\
\caption{The efficiency of the NN as a function of $\mu$ on the injected signal at $m_A=5000$, in signal region 8, for (a,c,e,g,i,k) $\epsilon=0.1$ and (b,d,f,h,j,l) $\epsilon=0.01$. 
  There are 5 lines corresponding to the $5$ different random samplings of the signal in the training of the NN; the network with the median efficiciency is also marked.
  Each signal is labeled by ($m_B,m_C$) in \GeV. (a,b) (80,80); (c,d) (80,200); (e,f) (80,400); (g,h) (200,200); (i,j) (200,400); (k,l) (400,400).
}
\label{fig:CWoLa:unblinded:NNeff_mu_5000}
\end{figure}

\FloatBarrier
\section{Unblinded Analysis: Fit Correction}
\label{app:CWoLa:fit_closure}
It is found that the distribution of significances of data with respect to the background fit both has a global (negative) offset and also has some dependence on $m_{JJ}$.
This is verified in the validation (inverted rapidity cut) data, as can be seen in Figure~\ref{fig:CWoLa:inverted:significance_dist}.
\begin{figure}[h!]
\centering
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{pulls_patience100_yinvertresample}.pdf}\label{fig:CWoLa:inverted:significance_offset}}
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{pulls_mjj_patience100_yinvertresample}.pdf}\label{fig:CWoLa:inverted:significance_mjj_fit}}\\
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{pulls_patience100_yinvertresample_UA2}.pdf}}
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{pulls_mjj_patience100_yinvertresample_UA2}.pdf}}
\caption{Distribution of significances in validation (inverted rapidity cut) data.
  (a,c) Overall distribution. The green dashed line shows the mean significance and the green dotted lines show the standard error on the estimate of the mean. The $t$-stat of this estimate is shown on the plot as well.
  (b,d) Dependence on $m_{JJ}$. The dots show individual observations across the $6$ signal region bins at each value of $m_{JJ}$ included in the fits. The red points with error bars bin these values and show the average in order to reduce the noise. The green dashed line shows the line of best fit, and the green dotted lines show the standard error on the fit. The $t$-stat associated with the estimate of the slope of the fit line is shown on the plot as well.
  (a,b) Using the nominal fit function (\ref{fitstep:1} and \ref{fitstep:2});
  (c,d) Using the UA2 fit function (\ref{fitstep:3}).
}
\label{fig:CWoLa:inverted:significance_dist}
\end{figure}
Only bins with fit values greater than $10$ are included, as the distribution of significances is not expected to be Normal for bins with low counts; see~\cite{Nachman:2012zf}.
The effect is measured both when restricting the fit to just the nominal fit functions (\ref{fitstep:1} and \ref{fitstep:2}) and when restricting the fit to just the UA2 fit function (\ref{fitstep:3}).

Some key statistics of the distribution of significances when using the nominal fit functions are given in Table~\ref{tab:validation:significance_dist}.
\begin{table}[htb]
  \centering
  \caption{Key statistics of distribution of significances in validation selection data.}
  \label{tab:validation:significance_dist}
  \begin{tabular}{c c}
    \hline
Statistic & Value   \\ \hline
Mean & -0.16 \\
Std. Dev. & 1.02 \\
Mean Std. Err. & 0.06 \\
$m_{JJ}$ Fit Slope& -0.20~\TeV$^{-1}$  \\
$m_{JJ}$ Fit Slope Std. Err.& 0.06~\TeV$^{-1}$  \\
$m_{JJ}$ Fit Intercept& 0.49 \\
    \hline
  \end{tabular}
\end{table}
The fit values are used to correct the background fit bin-by-bin given the $m_{JJ}$ value in that bin; i.e., the correction is exactly the green dashed line in Figure~\ref{fig:CWoLa:inverted:significance_mjj_fit}.
Since there is uncertainty on the estimate of the offset and mean from the validation dataset, and in addition an uncertainty on whether the fit derived in the validation dataset applies to the signal selection dataset, an uncertainty is applied on the offset and the slope independently according to the standard error of the estimate.

The question of how exactly to apply this correction is non-trivial.
The correction is derived on the significance of the data with respect to the background fit in the validation selection data, but it would be improper in general to set the median background fit value to the value that changes the significance by the correction amount, since then this correction would depend on the observed data in each bin.
Instead, the correction is applied to change the (approximate) median of the expected distribution of event counts under the Poisson hypothesis to have a significance of the corrected value.
In other words, given the background fit value $E_i$ and correction value $c$, the new fit value $E_i'$ is given by:
\begin{align}
  S(E_i',\left\lfloor E_i+1 \right\rfloor ) = S(E_i,\left\lfloor E_i+1 \right\rfloor)-c,
  \label{eqn:CWoLa:significance_corr}
\end{align}
where $S(E_i,O_i)$ is the significance as defined in Equation~\ref{eqn:CWoLa:significance_def}.
For the variations of the offset and the slope according to their uncertainties, new values of $E_i'$ are calculated according to the up and down variations of each.
The additional uncertainty on the background fit value $E_i'$ is given as the sum in quadrature of the differences due to these new values.

After applying this correction, the distribution of significances in the validation selection (inverted rapidity cut) data is shown in Figure~\ref{fig:CWoLa:inverted:significance_dist_corr}.
After the correction, the distribution of significances is consistent with mean $0$ across the entire range, indicating that the correction (Equation~\ref{eqn:CWoLa:significance_corr}) is working as intended.
\begin{figure}[h!]
\centering
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{pulls_corr_patience100_yinvertresample}.pdf}}
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{pulls_mjj_corr_vars_patience100_yinvertresample}.pdf}}\\
\caption{Distribution of significances in validation (inverted rapidity cut) data after applying the background fit correction.
  (a) Overall distribution, showing the up and down variations of the offset term. The green solid line shows the nominal mean significance and the green dashed/dotted lines show the down/up variations, respectively.
  (b) Dependence on $m_{JJ}$. The green solid line shows the line of best fit to the nominal significances, and the green dashed/dotted lines show the down/up variations on the slope term, respectively. The red points bin the individual nominal values and show the average, while the error bars indicate the same for the down/up variations of the slope term. 
}
\label{fig:CWoLa:inverted:significance_dist_corr}
\end{figure}

The correction is validated in the signal selection (no inverted rapididity cut) data in the sidebands of the fit, since after the NN selection it is expected there will not be any significant signal presence in the sidebands, and the background fit should describe the data there.
This validation is shown in Figure~\ref{fig:CWoLa:unblinded:significance_dist}.
\begin{figure}[h!]
\centering
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{pulls_outSR_patience100}.pdf}}
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{pulls_outSR_mjj_patience100}.pdf}}\\
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{pulls_outSR_corr_patience100}.pdf}}
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{pulls_outSR_mjj_corr_vars_patience100}.pdf}}\\
  \caption{Distribution of significances in signal selection (no inverted rapidity cut) data before (a,b) and after (c,d) applying the background fit correction.
  (a) Overall distribution. The green dashed line shows the mean significance and the green dotted lines show the standard error on the estimate of the mean. The $t$-stat of this estimate is shown on the plot as well.
  (b) Dependence on $m_{JJ}$. The dots show individual observations across the $6$ signal region bins at each value of $m_{JJ}$ included in the fits. The red points with error bars bin these values and show the average in order to reduce the noise. The green dashed line shows the line of best fit, and the green dotted lines show the standard error on the fit. The $t$-stat associated with the estimate of the slope of the fit line is shown on the plot as well.
  (c) Overall distribution, showing the up and down variations of the offset term. The green solid line shows the nominal mean significance and the green dashed/dotted lines show the down/up variations, respectively.
  (d) Dependence on $m_{JJ}$. The green solid line shows the line of best fit to the nominal significances, and the green dashed/dotted lines show the down/up variations on the slope term, respectively. The red points bin the individual nominal values and show the average, while the error bars indicate the same for the down/up variations of the slope term. 
}
\label{fig:CWoLa:unblinded:significance_dist}
\end{figure}
It can be seen both that before the correction, the distribution of significances and dependence on $m_{JJ}$ is consistent with that observed in the validation (inverted rapidity cut) data.
Some key statistics of the distribution are shown in Table~\ref{tab:unblinded:significance_dist}.
Both the mean significance and the slope of the significances with respect to $m_{JJ}$ are slightly higher than the values observed in the validation data.
\begin{table}[htb]
  \centering
  \caption{Key statistics of distribution of significances in signal selection data.}
  \label{tab:unblinded:significance_dist}
  \begin{tabular}{c c}
    \hline
Statistic & Value   \\ \hline
Mean & -0.10 \\
Std. Dev. &  1.06 \\
Mean Std. Err. & 0.06 \\
$m_{JJ}$ Fit Slope& -0.09~\TeV$^{-1}$  \\
$m_{JJ}$ Fit Slope Std. Err.& 0.07~\TeV$^{-1}$  \\
$m_{JJ}$ Fit Intercept& 0.19 \\
    \hline
  \end{tabular}
\end{table}

After the correction the distribution of significances is consistent with mean $0$ across the entire range to within the uncertainties, again with the nominal slightly higher since the correction is kept constant from the derivation in the validation selection data.
This indicates that the correction can also be applied to the bins in the signal regions.

\FloatBarrier
\section{Unblinded Analysis: Global Distribution of Significances}
\label{app:CWoLa:pulls}

The distribution of significances observed in the unblinded data in the given signal regions with no signal injected (Figure~\ref{fig:CWoLa:unblinded:stitched_fits} is studied under a toy Gaussian model.
In each bin, the significance $S_i$ that is being shown in the Figures is the same significance that goes into the $\chi^2$ calculation (Equation~\ref{eqn:CWoLa:signal_chi2}):
\begin{align}
S_i = \frac{O_i-E_i}{E_i}
\end{align}

Since the toy model being used is Gaussian, we only examine the significances in bins in which the background prediction $E_i>5$.
The empirical CDF $\Phi_O(x)$ of the observed significances $S_i$ is formed as:
\begin{align*}
  \Phi_O(x) = \sum_{i=1}^N\frac{\mathbbm{1}(x\le S_i)}{N}
\end{align*}
\noindent where $\mathbbm{1}$ is the indicator function, and the sum goes over all $N$ significances being considered; in this case $N=53$.

The toy model predictions are calculated by generating, in each toy, $N=53$ samples from a standard normal distribution and calculating the empirical toy CDF in the same way as for the observed.
$N_{\texttt{toys}}=20000$ empirical toy CDFs are generated in this way and quantiles across the toys are then calculated.

Figure~\ref{fig:CWoLa:cdf_pulls} shows the results of this test.
The observed empirical CDF lies well within the Gaussian expectation out to significance $x<2$.
For the most extreme excess observed in the data, this value or larger is observed in the toys at around the 1.3-$\sigma$ level, or $\sim10\%$ of the time.
\begin{figure}[h!]
\centering
\subfloat[]{\includegraphics[width=0.45\textwidth]{figures_CWoLa/{cdf_stitched_pulls_zoom_corr}.pdf}}
\caption{The CDF at each value $x$ for the observed data (orange) and for the toys (median is bold green dashed line and the green dotted lines correspond to (1,2)-$\sigma$ quantiles, respectively; the blue shadings correspond to finer quantiles at the $0.25\sigma$ level).}
\label{fig:CWoLa:cdf_pulls}
\end{figure}

\FloatBarrier
\section{Unblinded Analysis: Fits with Injected Signal}
\label{app:CWoLa:unblinded:fitsignal}
The fit results for all the signals at the injected $\mu$ value that gives rise to the limits given in Section~\ref{sec:CWoLa:unblinded:limits} can be found in Figure~\ref{fig:CWoLa:unblinded:fitsignal3000} for $m_A=3000$ GeV and in Figure~\ref{fig:CWoLa:unblinded:fitsignal5000} for $m_A$=5000 GeV.
For signals with no limit set (because the NN did not find that signal at any value of $\mu$), the fit at the maximum injected value (~\ref{tab:unblinded:injectedmu}) is shown.

\begin{figure}[h!]
\centering
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{q90_sigR5_Wprime_WZqqqq_M3000_m80_m80_nS1250_rs6_patience100_1.14.20_nomask_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{q99_sigR5_Wprime_WZqqqq_M3000_m80_m80_nS1500_rs4_patience100_1.14.20_nomask_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{q90_sigR5_Wprime_WZqqqq_M3000_m80_m200_nS750_patience100_1.14.20_nomask_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{q99_sigR5_Wprime_WZqqqq_M3000_m80_m200_nS1250_rs6_patience100_1.14.20_nomask_corr}.pdf}}\\
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{q90_sigR5_Wprime_WZqqqq_M3000_m80_m400_nS1000_patience100_1.14.20_nomask_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{q99_sigR5_Wprime_WZqqqq_M3000_m80_m400_nS850_rs3_patience100_1.14.20_nomask_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{q90_sigR5_Wprime_WZqqqq_M3000_m200_m200_nS350_rs4_patience100_1.14.20_nomask_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{q99_sigR5_Wprime_WZqqqq_M3000_m200_m200_nS225_rs5_patience100_1.14.20_nomask_corr}.pdf}}\\
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{q90_sigR5_Wprime_WZqqqq_M3000_m200_m400_nS600_patience100_1.14.20_nomask_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{q99_sigR5_Wprime_WZqqqq_M3000_m200_m400_nS600_rs6_patience100_1.14.20_nomask_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{q90_sigR5_Wprime_WZqqqq_M3000_m400_m400_nS500_rs6_patience100_1.14.20_nomask_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{q99_sigR5_Wprime_WZqqqq_M3000_m400_m400_nS350_rs6_patience100_1.14.20_nomask_corr}.pdf}}
\caption{The fit with an injected signal at $m_A=3000$, in signal region 5, for (a,c,e,g,i,k) $\epsilon=0.1$ and (b,d,f,h,j,l) $\epsilon=0.01$. The strength of the signal is the injected $\mu$ value that gives rise to the limits given in Section~\ref{sec:CWoLa:unblinded:limits}, or the maximum injected $\mu$ if no limits are set.
  Each signal is labeled by ($m_B,m_C$) in \GeV and $\mu$ for the two $\epsilon$ values. (a,b) (80,80), $\mu=$(1250,1500); (c,d) (80,200), $\mu=$(750,1250); (e,f) (80,400), $\mu=$(1000,850); (g,h) (200,200), $\mu=$(350,225); (i,j) (200,400), $\mu=$(600,600); (k,l) (400,400), $\mu=$(500,350). The red dashed lines indicate the fit uncertainty.}
\label{fig:CWoLa:unblinded:fitsignal3000}
\end{figure}

\begin{figure}[h!]
\centering
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{q90_sigR8_Wprime_WZqqqq_M5000_m80_m80_nS1000_rs5_patience100_1.14.20_nomask_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{q99_sigR8_Wprime_WZqqqq_M5000_m80_m80_nS1000_rs5_patience100_1.14.20_nomask_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{q90_sigR8_Wprime_WZqqqq_M5000_m80_m200_nS750_rs5_patience100_1.14.20_nomask_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{q99_sigR8_Wprime_WZqqqq_M5000_m80_m200_nS1000_patience100_1.14.20_nomask_corr}.pdf}}\\
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{q90_sigR8_Wprime_WZqqqq_M5000_m80_m400_nS1000_rs3_patience100_1.14.20_nomask_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{q99_sigR8_Wprime_WZqqqq_M5000_m80_m400_nS1000_rs3_patience100_1.14.20_nomask_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{q90_sigR8_Wprime_WZqqqq_M5000_m200_m200_nS75_patience100_1.14.20_nomask_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{q99_sigR8_Wprime_WZqqqq_M5000_m200_m200_nS150_patience100_1.14.20_nomask_corr}.pdf}}\\
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{q90_sigR8_Wprime_WZqqqq_M5000_m200_m400_nS280_rs4_patience100_1.14.20_nomask_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{q99_sigR8_Wprime_WZqqqq_M5000_m200_m400_nS750_rs6_patience100_1.14.20_nomask_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{q90_sigR8_Wprime_WZqqqq_M5000_m400_m400_nS50_rs4_patience100_1.14.20_nomask_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{q99_sigR8_Wprime_WZqqqq_M5000_m400_m400_nS50_rs4_patience100_1.14.20_nomask_corr}.pdf}}
\caption{The fit with an injected signal at $m_A=5000$, in signal region 8, for (a,c,e,g,i,k) $\epsilon=0.1$ and (b,d,f,h,j,l) $\epsilon=0.01$. The strength of the signal is the injected $\mu$ value that gives rise to the limits given in Section~\ref{sec:CWoLa:unblinded:limits}, or the maximum injected $\mu$ if no limits are set.
  Each signal is labeled by ($m_B,m_C$) in \GeV and $\mu$ for the two $\epsilon$ values. (a,b) (80,80), $\mu=$(750,280); (c,d) (80,200), $\mu=$(750,750); (e,f) (80,400), $\mu=$(1000,350); (g,h) (200,200), $\mu=$(75,75); (i,j) (200,400), $\mu=$(280,500); (k,l) (400,400), $\mu=$(50,50).
The red dashed lines indicate the fit uncertainty.}
\label{fig:CWoLa:unblinded:fitsignal5000}
\end{figure}

\FloatBarrier
\section{Unblinded Analysis: Dependence of Limits on $\mu$}
\label{app:CWoLa:unblinded:limits_mu}
The 95\% CL exlusion limits on the given signals as a function of $\mu$ are given in Figure~\ref{fig:CWoLa:unblinded:limits_mu_3000} for signals with $m_A=3000$ GeV and in Figure~\ref{fig:CWoLa:unblinded:limits_mu_5000} for signals with $m_A=5000$ GeV.
The limits for each of the $5$ different random samplings of the signal is included.
The network that gives rise to the median limit on the signal is also indicated, and the $\pm1\sigma$ and $\pm2\sigma$ bands and observed limit are indicated for this network; this is the network that is used to derive the final limits.
The envelope of the outputs across the different random samplings tends to be small when the NN efficiency is very high (when it is very low the limits are large and worse than existing limits), while the envelope widens at the $\mu$ values where the efficiency is middling; these are exactly the transition regions where the NN can find the signal but does not always.
The median limit tends to be smoothly falling with $\mu$ (before taking the $\max$ with the dotted line $\mu$), while single samplings from the envelope may not be; thus, the choice to run the analysis with different random samplings serves as a smoothing procedure.
For $m_A=5000$ GeV, for the best-performing signals at high $m_B,m_C$, the limits actually get worse with increasing $\mu$; this is related to the fact that the NN efficiency gets worse with increasing $\mu$ for these signals (Appendix~\ref{app:CWoLa:unblinded:NNeff_mu}).

\begin{figure}[h!]
\centering
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{limits95_mu_q90_M3000_m80_m80_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{limits95_mu_q99_M3000_m80_m80_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{limits95_mu_q90_M3000_m80_m200_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{limits95_mu_q99_M3000_m80_m200_corr}.pdf}}\\
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{limits95_mu_q90_M3000_m80_m400_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{limits95_mu_q99_M3000_m80_m400_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{limits95_mu_q90_M3000_m200_m200_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{limits95_mu_q99_M3000_m200_m200_corr}.pdf}}\\
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{limits95_mu_q90_M3000_m200_m400_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{limits95_mu_q99_M3000_m200_m400_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{limits95_mu_q90_M3000_m400_m400_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{limits95_mu_q99_M3000_m400_m400_corr}.pdf}}\\
\caption{The limits $\mu_{95}(\mu)$ as a function of $\mu$ on the injected signal at $m_A=3000$, in signal region 5, for (a,c,e,g,i,k) $\epsilon=0.1$ and (b,d,f,h,j,l) $\epsilon=0.01$. 
  There are 5 lines corresponding to the expected limit for the $5$ different random samplings of the signal in the training of the NN; the network with the median expected limit is also marked.
  The $\pm1\sigma$ and $\pm2\sigma$ bands and the observed limit are given for the network that gives rise to the median expected limit.
  The red stars indicate the expected, observed, and bands of the limit after taking the $\max$ of the limit and $\mu$.
  Each signal is labeled by ($m_B,m_C$) in \GeV. (a,b) (80,80); (c,d) (80,200); (e,f) (80,400); (g,h) (200,200); (i,j) (200,400); (k,l) (400,400).
}
\label{fig:CWoLa:unblinded:limits_mu_3000}
\end{figure}

\begin{figure}[h!]
\centering
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{limits95_mu_q90_M5000_m80_m80_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{limits95_mu_q99_M5000_m80_m80_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{limits95_mu_q90_M5000_m80_m200_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{limits95_mu_q99_M5000_m80_m200_corr}.pdf}}\\
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{limits95_mu_q90_M5000_m80_m400_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{limits95_mu_q99_M5000_m80_m400_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{limits95_mu_q90_M5000_m200_m200_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{limits95_mu_q99_M5000_m200_m200_corr}.pdf}}\\
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{limits95_mu_q90_M5000_m200_m400_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{limits95_mu_q99_M5000_m200_m400_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{limits95_mu_q90_M5000_m400_m400_corr}.pdf}}
\subfloat[]{\includegraphics[width=0.25\textwidth]{figures_CWoLa/{limits95_mu_q99_M5000_m400_m400_corr}.pdf}}\\
\caption{The limits $\mu_{95}(\mu)$ as a function of $\mu$ on the injected signal at $m_A=5000$, in signal region 8, for (a,c,e,g,i,k) $\epsilon=0.1$ and (b,d,f,h,j,l) $\epsilon=0.01$. 
  There are 5 lines corresponding to the expected limit for the $5$ different random samplings of the signal in the training of the NN; the network with the median expected limit is also marked.
  The $\pm1\sigma$ and $\pm2\sigma$ bands and the observed limit are given for the network that gives rise to the median expected limit.
  The red stars indicate the expected, observed, and bands of the limit after taking the $\max$ of the limit and $\mu$.
  Each signal is labeled by ($m_B,m_C$) in \GeV. (a,b) (80,80); (c,d) (80,200); (e,f) (80,400); (g,h) (200,200); (i,j) (200,400); (k,l) (400,400).
}
\label{fig:CWoLa:unblinded:limits_mu_5000}
\end{figure}

\FloatBarrier
\section{Computing Resources}
\label{app:CWoLa:computing}

The time for training depends on the signal region.
The initial factor of 3 networks to find the one with the lowest validation loss is done sequentially in a single job, but everything else is parallelized.
For signal region 4 (the largest), a single training takes $\mathcal{O}$(hours), and decreases proportionally according to the number of events in the signal region + sidebands.
For a single mass point, $\mu$ value, and signal region, we have $4\times 5 = 20$ such jobs.
For the no-signal networks, there are therefore $20\times 5$ jobs, some of which run very quickly.

There are far more jobs with injected signal.
We look at 12 signal mass points, and $\mathcal{O}(10)$ $\mu$ values for each, and for a given signal hypothesis only in a single signal region.
For the full unblinded analysis each training is repeated $5$ times with random samplings of the signal.
Therefore, for the full unblinded analysis, there are $4\times 5\times 12\times 10\times 5 = 12000$ jobs.
The Author is especially grateful for the \href{https://atlas.slac.stanford.edu/using-the-slac-computing-resourcesi}{computing resources at SLAC}, which provide a batch CPU system on which a single user can reliably run about 400 jobs simultaneously.
Altogether it takes $\mathcal{O}$(days-weeks) in compute time to reproduce all the results in this analysis. 

Each job stores $\mathcal{O}(100)$ MB of data in output, since there are tens of millions of events for which a few key features have to be stored, e.g. the NN output and the values of the features and $m_{JJ}$.
Probably wastefully, each step of the validation combination process is stored as a new file, in order to reduce computation time, increasing the amount of storage by a factor of $\sim 2$.
Therefore the whole analysis uses a few TB in storage space, which again we are grateful to the SLAC computing facilities for providing and maintaining.
